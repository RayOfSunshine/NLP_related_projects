### 文本匹配

#### 基于特征的方式

+ tf-idf，BM25，词法等方面

+ 依赖人工特征，和不断试错的方法，泛化能力就显得比较一般，而且由于特征数量的限制，导致参数量受到限制，模型的性能比较一般。

  

#### 深度学习下的文本匹配

+ 基于表征的匹配

  + <font color=red>原则：初始阶段对两个文本各自单独处理，通过深层的神经网络进行编码，得到文本的表征，然后基于得到的文本表征，采用相似度计算的函数得到两个文本的相似度。</font>
  + DSSM：确定了建模流程：Embedding层 -> Encoding层 -> DNN层 -> Prediction层
  + CDSSM：将MLP替换成CNN，并提取n-gram特征
  + 能够优化的方向：
    + Embedding 层是固定的
    + Encoding 层无非在加上各种 char-embedding，或者 entity-embedding 来引入先验知识
    + 稍微有点创新的就只有DNN层，但是由于表征模型从头到尾对两个带匹配文本都是独立处理的，能做的只能是怎么得到更好的表征向量，很容易想到的就是把DNN替换为RNN型网络或者后来的Attention网络
    + Prediction层则是寻找不同的相似度计算函数，或者直接使用一层线性层代替
  + ARC
  + Siamese Network
  + 近期baseline：基于bert的双塔模型

+ 基于交互的匹配

  + 原则：认为在最后阶段才计算文本的相似度会过于依赖表征的质量，同时也会丢失基础的文本特征（比如词法、句法等）。<font color=red>也就是说但是文本中单词的关系、句法的特征，高层的表征比较难捕获。</font>所以提出尽可能早的对文本特征进行交互，捕获更基础的特征，最后在高层基于这些基础的特征进行匹配。
  + ARC II
    + 首先，对输入语句得到每个单词的Embedding，然后经过一层的CNN得到两个句子N-gram级别的表征（这个使用多个不同大小的卷积核，得到多个N-gram级别表征）
    + 接着计算基于每一个N-gram级别的表征计算交互矩阵（即一个句子中某一个位置的向量和另一个句子中其他位置的向量计算相似度，可以是点积或者cosine相似度），并在channel维度上进行拼接得到4层的张量
    + 对上面得到的4层张量采用2D-CNN，再经过max-pooling得到最终的表征
    + Flatten之后经过MLP得到最终的匹配分数。
  + MV-LSTM
    + 主要是使用Bi-LSTM对Embedding进行强化编码
  + MatchPyramid
    + 提出了计算交互矩阵时多种匹配模式（**「Indicator, Cosine, Dot」**）。
  + DRMM
    + 在检索任务中更关注相关性匹配，关键词起到重要作用
    + 计算得到匹配矩阵之后，采用match histogram的方式将query中每个单词的相似度的值映射到不同的bin中，`[1, 1]`这个bin表示exact match，`[-1, 0)`区间均匀划分得到的每个bin都表示soft match
    + 使用直方图中的计数作为向量中每一维的值，得到每个单词编码后的向量 ，在经过线性层得到每个单词的匹配分数
    + 对于query中每个单词的向量，使用Term Gating网络计算权重，对上面单词的匹配分数进行加权得到最终匹配分数
  + <font color=red>Attention的使用方向主要集中在两点</font>：
    + 一个是Embedding之后的Encoding层，通过Attention来得到强化单词的表征；
    + 一个是使用交互匹配矩阵得到两个文本align之后的Cross Attention。采用什么样的Attention函数，以及如何组合不同Attention之后的结果就成为导致各个模型性能不同的关键点。
  + BiMPM：
    + Word Representation层
      + 有Word Embedding和Char Embedding经过LSTM之后的Word表征拼接得到单词表征
    + Context Representation层
      + 使用一个BiLSTM对两个文本进行Encoding，得到每个word的前向和后向表征。
    + Matching 层
      + **Full Matching**：每个text中单词的前向Embedding和另一个text正向LSTM最后一个hidden state向量匹配得到 维向量，每个后向Embedding和另一个text反向LSTM的最后一个hidden进行匹配得到 维向量。
      + **MaxPooling Matching**：每个text中单词的前向（后向）上下文Embedding和另一个text中单词的前向（后向）Embedding匹配，对匹配结果max-pooling，这样每个单词前向（后向）分别得到 维向量；
      + **Attentive Matching**：计算一个text中单词前向（后向）Embedding和另一个句子中每个单词前向（后向）Embedding的相似度，取相似度值作为加权向量权重得到Attentive向量，然后使用Attentive向量和该text的对应单词向量进行匹配。
      + **MaxAttentiveMatching**：取另一个句子每个维度最大值作为Attentive向量，按照上面的方式进行匹配
    + **Aggregation层**
      + 通过Matching层得到每个句子中单词的强化表征，对两个句子分别使用BiLSTM编码，将两个方向最后一个Hidden states拼接，得到最终向量。
    + **Prediction层**：
      + 将上面得到的向量，经过MLP输出最终的分数
  + 基于交互的网络结构：**Embedding层 -> Encoding层 -> Matching层 -> Aggregation层 -> Prediction层**
    + Embedding层获取单词的表征
    + Encoding层对单词表征进行强化编码
    + Matching层对带匹配的两个文本计算交互特征
    + Aggregation层对不同的交互策略进行融合
    + Prediction层基于融合的向量计算最终的概率
  + ESIM
    + 具体见专门的md
  + DeepRank
    + 由于文本检索任务中，`text_right`通常是倒排索引返回的document，属于长文本，之前的研究直接将其作为输入，一方面增加了计算复杂度，另一方面注入了很多无效信息。
    + 考虑标注人员对文档相关性数据进行标注的流程，即<font color=red>相关位置检索 -> 确定局部相关性 -> 聚合局部相关性输出相关性标签</font>
    + 提出通过获取每一个query-term对应document中query-centric context，来避免过多无效信息和复杂计算。

  + HCAN:
    + 在Matching层分别计算Relevance Matching和Semantic Matching，这样可以很好地解决信息检索中exact match和soft match的问题
    + 采用了花式 Attention 和 Pooling 策略
  + Bert
    + 直接使用`[CLS]`输出向量分类效果好
  + 更轻量级的模型；
    + RE2
    + Enhanced-RCNN

+ 评估方法：

  + 分类使用损失函数：CrossEntropy
  + 排序使用损失函数：HingeLoss
  + 常用的评价指标：MRR，NDCG，准确率，召回率
  + https://zhuanlan.zhihu.com/p/111763741

+ matchzoo

  + https://github.com/NTMC-Community/MatchZoo-py
  + https://zhuanlan.zhihu.com/p/94085483
  + https://zhuanlan.zhihu.com/p/98180757