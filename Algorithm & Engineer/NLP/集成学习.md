## 集成学习

将多个分类器的结果统一成一个最终的决策，每个单独的分类器成为基分类器

#### 集成学习的类型

+ 分类器错误
  + 偏差：分类器的表达能力有限导致的系统性错误，表现为训练误差不收敛
  + 方差：对于样本分布过于敏感，导致样本较少时产生过拟合

+ boosting（<font color=red>迭代式学习</font>）
  + 串行方式，各分类器之间有依赖
  + 层层叠加，每一层在训练时对前一层分类器分错的样本给予更高的权重。
  + 测试时，对各分类器的结果进行加权得到最终结果
  + <font color=red>逐步聚焦分错的样本，减小集成分类器的偏差</font>
+ bagging（<font color=red>集体投票决策</font>）
  + 并行训练，各分类器之间没有依赖
  + 数据集分成多个子集，每个子集对应于一个模型，也就是集体决策
  + 在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最终判断
  + <font color=red>对样本进行多次采样，多个模型做综合来减小方差</font>

#### 集成学习的步骤和例子（以Adaboost为例）

+ 找到误差相互独立的基分类器
  + 常用基分类器为ID3决策树作为基分类器，树形结构比较简单且较易产生随机性
+ 训练基分类器
  + 初始化采样分布（1/N），准备数据集
  + 多个基分类器循环
    + 选择子集
    + 从子集中训练出基分类器
    + 计算该基分类器的错误率
    + 根据上述错误率，计算该基分类器下的权重 $a_t = log \frac {(1-e_t)}{e_t}$
    + 设置下一次采样的分布，并将其归一化为一个概率分布函数
+ 合并基分类器的结果（voting针对并行，stacking针对串行）
  + 给定一个位置样本z，输出分类结果为加权投票的结果
  + 对正确分类的样本降低权重，多分类错误的样本加大权重或保持不变。
  + 模型融合时也根据错误率对基分类器进行加权融合
+ <font color=red>梯度提升决策树（GBDT）</font>
  + 每一颗树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量

#### 常用的基分类器

+ 很多机器学习模型都使用决策树作为基分类器的原因
  + 能够将样本权重整合到训练过程中，而不需要使用采样的方式调整样本权重
  + 其表达能力和泛化能力能够通过调节树的层数来做折中
  + 样本扰动对决策树影响较大，不同子集对应的基分类器结果随机性较大，不稳定学习器更适合作为基分类器
+ 神经网络也适合作为基分类器
  + 可通过调整神经元数量，连接方式，网络层数，初始权重等方式引入随机性

#### 随机森林中基分类器的选择

+ 随机森林属于Bagging类集成学习，需要保证集成后的方差要小于基分类器的方差，基分类器要对样本敏感，即不稳定分类器
+ 相比决策树，线性分类器和K-近邻都是稳定分类器，方差本身不大，不适合用作基分类器

#### 偏差和方差

能够根据偏差和方差两个指标来指导模型的优化

+ 偏差：
  + 采样后大小为m的数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差
  + 通常由于对学习算法的做了错误的假设所导致的
  + 这种误差体现在训练误差上
+ 方差：
  + 采样后大小为m的数据集训练出的所有模型的输出的方差
  + 通常由于模型的复杂度太高
  + 这种误差体现在测试误差相对于训练误差的增量上

#### Bagging降低方差，Boosting降低偏差的原因

+ Bagging
  + bootstrap aggregating，再抽样，每个样本上训练出来的模型取平均
  + 从公式角度理解，n个随机变量，方差为$\sigma^2$，两两变量之间的相关性为$\rho$，那么n个随机变量的均值的方差就是$\rho*\sigma^2 + (1-\rho)*\sigma^2/n$。当n个随机变量相互独立时，$\rho$为0，那么方差就等于$\sigma^2/n$，方差减小为原来的n分之1.
  + 模型之间不可能完全独立，所以在随机森林算法中，每次选取的节点分裂属性时，会随机抽取一个属性子集，而不是从所有属性中选取最优属性，就是为了避免弱分类器之间过强的相关性
  + 也可以通过重采样的方式弱化相关性
+ Boosting
  + 上一层的错误和方差作为下一层的输入，使得模型偏差不断降低
  + 其分类器之间是强相关的，所以不会减小方差
+ 泛化误差，方差，偏差，和模型复杂度的关系
  + 模型复杂度越高，方差越大；模型复杂度越低，偏差越大
  + 泛化误差随着方差和偏差变化，谁高就随谁保持同步变化

#### 梯度提升决策树（GBDT）

+ Gradient Boosting Decision Tree，是基于决策树预测的残差进行迭代的学习，是boosting模型
+ 梯度提升：根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式（需要训练一个权重，也就是加权）结合到现有的模型中。
+ GBDT有时又被称为MART，GBDT使用的决策树为CART
+ <font color=red>利用残差进行训练的，在预测的过程中，也需要把所有树的预测值加起来，得到最终的结果</font>

#### 梯度提升和梯度下降的区别和联系

+ 相同点：在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息对模型进行更新
+ 不同点：
  + 梯度下降：模型以参数化的形式表示，模型的更新等价于参数的更新
  + 梯度提升：模型是定义在函数空间中的，具体使用的模型种类可以扩展

#### GBDT的优点和局限性

+ 优点
  + 预测阶段计算速度快，树与树之间可并行化计算 
    + <font color=red>问题：预测阶段怎么并行？</font>
  + 在分布稠密的数据集上，泛化能力和表达能力都很好
  + 采用决策树作为弱分类器具有较好的解释性和鲁棒性，能够自动发现一些高阶特征，并且也不需要对数据进行预处理
+ 局限性
  + 在高维稀疏数据集上表现不如神经网络
  + 处理文本特征的能力不如处理数值特征
  + 训练阶段需要串行，只能通过局部并行的方式提高训练速度

#### XGBoost与GBDT的区别和联系

+ GBDT是机器学习算法，XGBoost是该算法的工程实现
+ 使用CART作为基分类器，XGBoost显式地加入了正则项来控制模型的复杂度，防止过拟合，提高泛化能力
+ GBDT训练时只是使用了损失函数的一阶导数信息，而XGBoost同时使用一阶和二阶导数
+ 传统的GBDT采用CART作为基分类器，而XDBoost支持多种类型的基分类器，比如线性分类器
+ 传统的GBDT在每轮迭代时使用全部的数据，而XGBoost可以对数据进行采样，与随机森林类型
+ 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略

