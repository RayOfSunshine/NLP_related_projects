生成式任务的解码：

+ 模型的输出按照时间步依次获得的，后面时间步的输出依赖于前面时间步的输出
+  为了生成完整的句子，需要一个称为解码的额外动作来融合模型多个时间步的输出，使得最终得到的序列的每一步条件概率连乘起来最大
+ 文本生成任务，每一个时间步可能的输出种类称为字典大小，值为5000-6000，即常用汉字的个数，在如此大的基数下，遍历整个生成空间是不现实的。
  + 类似于全局的维特比解码就不适合，因为其是全局的解码方法
+ 贪心策略：
  + 每一个时间步下都取出一个条件概率最大的输出，再将从开始到当前步的结果作为输入去获得下一个时间步的输出，直到模型给出生成结束的标志。
  + 这样做将原来指数级别的求解空间直接压缩到了与长度线性相关的大小。由于丢弃了绝大多数的可能解，这种**关注当下**的策略无法保证最终得到的序列概率是最优的。
+ beam search
  + 放宽稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的**1**个输出，而是保留**num_beams**个。
  + 第一个时间步，从总数5个结果中获得最优两个结果，其余三个结果被抛弃（num_beams = 2）
  + 第二步基于每个这两个结果继续生成，每个结果生成5个候选，然后将这10个候选进行统一排名，再保留最优的两个，第三步同理
  + <font color=red>beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种牺牲时间换性能的方法。</font>
  + 