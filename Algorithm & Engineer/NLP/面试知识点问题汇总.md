如何准备场景题：

1. 特定，垂直场景下的特征提取（特征工程）
   + 那些特征可以提取，如何提取



问题汇总：

2. 神经网络为什么会产生梯度消失现象？
4. 挑一种激活函数推导梯度下降的过程？
3. Attention都有哪些变种？
6. 如何将attention机制引入关系抽取的？
   + 见关系抽取模型
7. 为什么要引入attention？
   + 见关系抽取模型
8. 如何部署？
8. t-检验
9. GBDT：梯度提升决策树
10. kmeans公式推导，证明kmeans极值点数学表达
11. MLE(maximum likelihood estimation) 和MSE (mean square error)的关系，对高斯函数求个log就出来了
12. xgb和gbdt的区别 (几乎必问的题目，提前准备一下，说的要有条理，有哪些[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)优化，哪些工程实现优化，可以适当扩展提一下lgb)
13. || x ||范式是什么意思？
14. 正则化是什么意思？有什么用？
    + 在反向传播时存在于损失函数中，能够减小权重的幅度，防止过拟合
15. 调参流程
16. 多任务学习
17. 如果多任务学习输出的y之间有相互制约的关系，应该使用CRF进行处理
18. 多模态学习
19. 还需要再看一下GNN相关的内容
20. 如果数据不充足，或者不平均，要怎么解决？从数据增强和建模的方面解释
21. 集成学习两种方式对比
22. 左轮手枪可以装6颗子弹，两颗子弹相邻装在一起，现在别人打了一枪没死，你选择接着打还是随机转一圈再打。
23. 艾滋病患病概率1/100，误诊概率1/100，一个人诊断患病，则实际患病的概率
    + 贝叶斯公式，假设真实患病的概率为A，确诊患病的概率为B，实际要求的是$P(A|B)$，使用贝叶斯公式：$P(A|B) = \frac {P(A)P(B|A)}{P(A)P(B|A) + P(\bar A)P(B|\bar A)}$。误诊的概率其实是$P(B|\bar A), P(\bar B|A)$，那么诊断正确的概率就是$P(\bar B|\bar A), P(B|A)$，患病的概率就是 $P(A)$，结果为0.5
24. 生了俩孩子，有一个女的，另一个是男的概率
25. 进程和线程
26. 模型蒸馏
27. 特征工程的常见手段（去噪等）
28. 讲一下roc曲线和AUC值；真阳性和假阳性怎么算的？
29. 讲一下改进的tf-idf
30. 显存不够怎么解决？
31. 100个python的问题
32. bert 字向量的embedding怎么训练得到的
33. layer normlization 和 batch normlization
34. 为什么Adam有时候可能没那么好，怎么解决
35. 词向量负采样的细节，数学原理
36. 问大规模的用户导致维度爆炸怎么处理
37. 了解什么评估指标。说了recall，precision，f1-score以及AUC，手动实现AUC
38. 英语流利说面试
    + 使用关键词提取，和人工标注关键词的方法，使用起来各有什么优缺点
    + 如何评估实体识别和关系抽取的结果（除了常见的准确率评估外）
      + ROC，AUC，PR曲线
    + CRF为什么能做到全局考虑，什么原理和模型？计算路径分数和，
    + 负采样有什么作用，负采样的方法
39. 叮咚买菜
    + 如何评估实体识别和关系抽取的结果（除了常见的准确率评估外）
    + 哪两种mask的方式
    + 手写multihead-attention的算法（Q，K，V）
    + 使用过的模型的改进空间：
      + 关系抽取的句子级别attention中的不同batch-size，预测结果差别较大
      + 两个假设，两类实体，一种关系，可以使用细粒度情感分析的方式进行优化
      + 抽取完成后，将这个关系同样应用在同类型的其他实体之间（增量）
    + adam优化器的优劣
40. 英语流利说
    + 其他能够解决图谱补全的方法（基础模型，原理及与RGCN的区别，模型选型）
    + RGCN的模型结构，公式及评估方式（评估方式还是要看一下论文）
    + 字典树
41. 云从
    + 最长回文子串
    + 最长回文子序列
    + 需要看一下实体识别，关系抽取的评估方式，对于识别或抽取不准的情况应该如何处理
    + 实体类型，关系类型，各自数量有多少？
    + 外部找到的结构化数据如何与本地产生的数据进行融合
    + 问答结果中识别到的关键词如果不能与词表中的任何词进行映射怎么办？
      + 用于输入问题中关键词表达模糊，如何做<font color=red>实体对齐</font>
    + 为什么选用使用BiGRU而不使用BiRNN（这个问题感觉是在问GRU比RNN的优势，回答没尝试就太没水平了）
42. 比心陪练
    + 垃圾公司，毫无收获
43. 滴滴：
    + 实体识别结果的评价方式（面试官说使用精确率，召回率可能不太准确）
    + 实体对齐的问题
44. 拼多多
    + 项目方面：
      + 知识图谱：实体对齐，实体消歧
      + 实体识别：基于词表的数据标注，训练出来的基本收敛于词表，如何发现新词
      + 关系抽取：为什么使用GRU，为什么不都使用LSTM，为什么要区别对待
      + RGCN：预测潜在关系的三元组向量的乘积，复杂度比较高
    + bert
      + 三个embedding向量：输入词向量怎么得到
      + 为什么bert和transformer的position embeddig方式不一样
      + 





### embedding相关

1. embedding的方法？

+ 词袋模型：
  + one-hot
  + tf-idf
  + TextRank
  + <font color=red>问题</font>：维度灾难，语义鸿沟
+ 主题模型
  + 分布式表示
  + LSA
  + LDA
  + <font color=red>矩阵分解问题</font>：利用全局语料特征，但SVD求解计算复杂度大；
+ 基于词向量的静态表示
  + 分布式表示
  + word2vec
    + skip-gram：根据中间词预测上下文
    + CBOW：根据上下文预测中间词
      + 参数个数：包含两个参数矩阵，第一个参数矩阵维度为[V, N]，用来将多个维度为V的词向量转化为多个维度为N的向量，这些向量通过加权平均后（这里可以获取到预测词的词向量），与第二个维度为[N, V]的参数矩阵相乘，得到维度为V的预测词
    + 词嵌入可将词的特征映射到较低的维度，使用模型参数更少，训练更快。
    + 将onehot编码，通过与参数矩阵乘积，转换成embedding
    + 问题：优化效率高，但是其是基于局部语料的
    + <font color=red>对比 NNLM</font>：
      + 本质都可以看作是语言模型；
      + 词向量是NNLM的产物，而word2vec专注于词向量本身
      + 预测当前词时，是上下文词降维后直接相加的，而不是拼接，也没有隐层
      + 使用 hierarchical softmax 和 negative sampling 进行优化，不再遍历整个词表
        + hierarchical softmax：生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小
        + negative sampling：降低每一个词负样本的预测概率，加快运算；
    + <font color=red>对比 fastText</font>
      + 都可以无监督学习词向量， fastText训练词向量时会考虑subword
      +  fastText可以进行有监督学习，进行文本分类
        + 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径
        + 引入N-gram，考虑词序特征
        + 引入subword来处理长词，处理未登陆词问题
    + <font color=red>对比 Glove</font>
      + word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于<font color=red>全局语料</font>的，可见glove需要<font color=red>事先统计共现概率</font>；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。
      + word2vec是无监督学习，实际上glove还是有label的，即共现次数$log(X_{ij})$
      + word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。
      + **glove可以被看作是更换了目标函数和权重函数的全局word2vec**
  + GloVe
    + 基于全局语料库，并结合上下文语境构建词向量，结合了LSA和word2vec的优点
    + 构建过程
      + 根据语料库构建一个共现矩阵，矩阵中的每一个元素 $X_{ij}$ 代表单词 ![[公式]](https://www.zhihu.com/equation?tex=i) 和上下文单词 ![[公式]](https://www.zhihu.com/equation?tex=j) 在特定大小的上下文窗口内共同出现的次数。
      + 构建词向量和共现矩阵之间的近似关系，目标函数的基本形式就是平方差损失函数，在其基础上添加了一个权重函数
    + 训练过程：
      + 虽然不需要人工标注，但还是有标签的，就是共现次数$log(X_{ij})$
      + 向量 $\omega$，$\bar \omega$ 为学习参数，本质上与监督学习的训练方法一样，采用了AdaGrad的梯度下降算法，对矩阵 X 中的所有非零元素进行随机采样，学习曲率（learning rate)设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。
      + 最终学习得到的是两个词向量是 $\bar \omega$ 和  $\omega$，因为 X 是对称的（symmetric），所以从原理上讲 $\bar \omega$ 和 $\omega$ ，是也是对称的，<font color=red>他们唯一的区别是初始化的值不一样，而导致最终的值不一样 </font>。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和 $\omega + \bar \omega$ 作为最终的vector（<font color=red>两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性</font>)。
    + 可以进行词向量的动态表征
    + 对比<font color=red> LSA </font>
      + LSA 基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高；glove可看作是对LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；
+ 基于词向量的动态表示：
  + 解决静态表示不能解决的<font color=red>一词多义</font>的问题

  + elmo

  + GPT

  + bert

    + https://www.jianshu.com/p/4dbdb5ab959b
    + 全称：Bidirectional Encoder Representation from Transformers
    + 核心：**Transformer Encoder**
    + **为什么bert采取的是双向Transformer Encoder，而不叫decoder？**：
    
      + BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。
    + **bert不使用拼接两个反向的transformer decoder的原因**

      + BERT 的作者认为，这种拼接式的bi-directional 仍然不能完整地理解整个语句的语义。更好的办法是用上下文全向来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。
    + <font color=red>bert为什么要采取Marked LM，而不直接应用Transformer Encoder？</font>
    
      + 深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。
      + 为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。
    + <font color=red>bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？</font>
    
      + <font color=red>预训练和fine-tuning之间不匹配，因为在fine-tuning期间从未看到被[MASK]的token。</font>为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。80％的情况用[MASK]标记替换单词；10％的情况用一个随机的单词替换该单词；10％的情况保持单词不变。==原因1：==<font color=red>这样做的目的是将表示偏向于实际观察到的单词（后续微调时输入的数据并不包含mask）</font>。==原因2：==<font color=red>Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示，同时也赋予了bert一定的纠错能力</font>。因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。
    + <font color=red>每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。</font>团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。
    + next sentence prediction
      + 给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后
    
  + 三者区别：

    + | 模型        | elmo                           | GPT               | BERT            |
      | ----------- | ------------------------------ | ----------------- | --------------- |
      | 特征提取器  | LSTM                           | Transformer       | Transformer     |
      | 层数        | 1层静态词向量+2层LSTM          | 多层              | 多层            |
      | 语言模型    | 双向（两个方向相反的单向拼接） | 单向              | 双向            |
      | transformer | 不采用transformer              | decoder句子不完整 | encoder句子完整 |
      
    + bert 输出：<font color=red>输出句子向量的语义表征信息</font>

      + last_hidden_state：shape是(batch_size, sequence_length, hidden_size)，hidden_size=768,它是模型最后一层输出的隐藏状态，序列标注通常用。
      + pooler_output：shape是(batch_size, hidden_size)，是last_hidden_state[0]。这是序列的第一个token(classification token)的最后一层的隐藏状态[CLS]的向量表征（基础版分类用的这一层）
      + hidden_states：这是输出的一个可选项，如果输出，需要指定 config.output_hidden_states = True, 它也是一个元组，它的第一个元素是embedding，其余元素是12层transformer各层的输出，每个元素的形状是(batch_size, sequence_length, hidden_size),共计13个元素，最后一个元素是last_hidden_state。
      + attentions：这也是输出的一个可选项，如果输出，需要指定config.output_attentions=True,它也是一个元组，它的元素是每一层的注意力权重，用于计算self-attention heads的加权平均值

    + 缺点：

      + MLM直接对<font color=red>单个token进行随机mask，丢失了短语和实体信息</font>，这一点对中文尤其明显，百度ERNIE对其进行了改进
      + MLM仅预测被mask的token，<font color=red>其他token没有参与到预测中，导致MLM训练效率偏低，训练时间过长。</font>ELECTRA围绕这一点进行了改进。
      + <font color=red>NSP任务的负样本构造过于简单，导致模型不能充分训练，NSP对下游任务的作用比MLM要小。</font>SpanBERT、ALBERT、Roberta均提到了这一点，并进行了相关改进，或者干脆弃用NSP。

  + ALBERT:  A Lite BERT

    + 减小参数量：
      + <font color=red>嵌入参数化进行因式分解</font>。大的词汇嵌入矩阵分解为两个小的矩阵，将隐藏层的大小与嵌入层的分离开。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，先映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)
      + <font color=red>跨层参数共享</font>。这一技术可以避免参数量随着网络深度的增加而增加。
    + 训练任务
      + <font color=red>Sentence-order prediction</font> (SOP) 来取代NSP。其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，<font color=red>模型学习到的就更多是句子间的连贯性</font>。用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。

  + Robert 

    + 改进优化函数
    + 采用动态掩码，每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。而BERT预处理好一个掩码模型，后面直接调用
    + 使用更大的数据集，更大的batch
    + 使用BPE（Byte-Pair Encoding）来处理文本，是字符级和词级别表征的混合，用更大的 byte 级别 BPE 词汇表，且没有对输入作任何额外的预处理或分词。
    + 去掉了NSP任务，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES）

  + ERNIE

    + 利用短语和实体级别的mask方式，来融合外部知识
    + 添加更多优质中文语料
    + DLM。对Dialog的角色，进行了Dialog embedding，从而加强模型在Dialog上的效果



### 通用模型相关

CNN：

+ 稀疏交互（局部连接）
  + 意义：文本数据可能包含局部特征，可以选择先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征
  + 优势：优化过程中的时间复杂度将会减小几个量级；一定程度上避免过拟合
+ 参数共享（卷积运算的固有属性）
  + 意义：使得卷基层具有平移等变性。可以简单理解为同一种特征不论出现在任意位置都应该能够被识别。神经网络的输出对于平移变换来说应该是等变的
  + 优势：降低模型的存储需求
+ 池化
  + 本质：降采样
  + 平均池化：
    + 作用：能够抑制由于邻域大小受限造成估计值方差增大的现象
  + 最大池化
    + 作用：能够抑制网络参数误差造成估计均值偏移的现象
  + 相邻重叠区域池化：
    + 采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠的区域
  + 空间金字塔池化
    + 考虑多尺度的信息，同时计算1\*1，2\*2，4\*4的矩阵的池化并将结果拼接在一起作为下一层的输入
  + 作用
    + 显著降低参数量， 还能够保持对平移，伸缩，旋转操作的不变性
    + 平移不变性：输出结果对输入的小量平移基本保持不变
    + 伸缩不变性：经过尺度变换后，大概率保证最大池化的值不变。因为神经元感受的是邻域输入最大值，而非某一个确定值
    + 旋转不变性：多个过滤器分别学习不同方向的输入数据，即使输入数据方向改变了，也不会影响池化响应的结果
+ 用于文本分类任务
  + 输入层
    + 词向量可以提前在其他语料中训练好，优势是能够获得很多先验的知识
    + 或者作为参数在CNN中进行训练，优势是能够获取与任务相关的特征
    + 输入层可以同时采用两种方式，一种使用先验训练好的词向量，在训练过程中不再更新；另一种将训练好的词向量拿来初始化，跟随模型一起训练
  + 卷积层
    + 定义大小不同的滑动窗口进行卷积操作，在窗口上得到的卷积结果进行拼接
    + 每一次卷积相当于一次特征提取
  + 池化层
  + 全连接
+ 深度残差网络
  + 网络层数加深带来的问题
    + 优化函数越来越陷入局部最优解
    + 梯度消失的问题更加严重，梯度在反向传播时衰减严重
  + 主要解决：
    + 梯度消失问题
    + 经过两层得到的变换是F(x)，加上离输入近的神经网络层（因为其距离输出层较远，训练比较难，这里直接进行短接）x，得到最终的输出就是H(x) = F(x) + x
    + F(x)只需要拟合输入x与目标输出H(x)的残差--H(x) - x
    + 如果某一层的输出已经很好的拟合了期望结果，那么多加入一层（<font color=red>应该是加到已有层的前面</font>）不会使得模型变得更差，因为多加入的这层的输出将直接被短接到两层之后，相当于直接学习了一个恒等映射，而跳过的两层（即上面的F(x)）只需要拟合上层输出（上面的x）和目标之间（即上面的H(x)）的残差



### 实体识别模型

1. 为什么实体识别模型要使用BiLSTM+CRF，CRF起到的作用是什么？

+ BiLSTM本身已经能够预测文本序列与标签之间的关系，而不能预测标签与标签之间的关系，标签之间的相互关系就是CRF中的转移矩阵；CRF中会通过转移矩阵计算每个序列路径的打分情况，将得分最高的那条路径序列作为最终的结果；CRF能够在BiLSTM的预测结果的基础上，根据句子的约束条件，修正预测结果，保留最佳路径。

2. 简述转移矩阵和状态矩阵

+ 转移矩阵：由上一个预测的实体类别到下一个实体类别的转移分数组成，在训练模型时，可以对这个转移矩阵进行随机初始化，这些分数会在训练中被更新，也就是说CRF可以自己学习到
+ 状态矩阵：由单词的位置索引和实体类型索引组合，用来记录某个单词被预测为某个实体类型的分值，这些分值来自于BiLSTM层的输出

3. 为什么要使用BiLSTM，相比LSTM有什么优点，有什么共同点？

+ LSTM在句子建模时的优点：通过词向量相加（或均值）得到的句向量不能表示词之间的顺序，但是<font color=red>LSTM能够更好捕捉到较长距离的依赖关系</font>，可以通过训练过程学习到应该记忆哪些信息和忘记那些信息
+ 优点：由前向LSTM与后向LSTM组合而成，可以更好的捕捉双向的语义关系
+ 共同点：两者在自然语言处理任务中都常被用来建模上下文信息

+ 句子编码方式依赖于对向量拼接，不同的拼接结果对应着不同的目的。
  + 如果需要取出对于每个词的向量，那么需要将每个位置，不同方向的LSTM的输出结果进行拼接（concat），如下图所示：
  + 三个词的编码结果分别为，$[h_{L0}, h_{L1}, h_{L2}]$ 和 $[h_{R2}, h_{R1}, h_{R0}]$ ，那么最终的编码方式为 $[h_0, h_1, h_2]$，其中$h_0=[h_{L0}, h_{R2}]$,  $h_1 = [h_{L1}, h_{R1}]$,  $h_2=[h_{L2}, h_{R0}]$

![截屏2021-05-24 上午10.16.58](/Users/lixuanhong/Desktop/Materials/截屏2021-05-24 上午10.16.58.png)

+ 
  + 如果需要获取对句子的编码，则使用 $[h_L2, h_R2]$ 的拼接结果，因为这个包含了前向和后向的所有信息，如下图所示

    ![截屏2021-05-24 上午10.30.05](/Users/lixuanhong/Library/Application Support/typora-user-images/截屏2021-05-24 上午10.30.05.png)

5. GRU与LSTM的区别：

+ 它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态。最终的模型比标准的LSTM要更简单



### 工程或训练模型相关：

1. ab-test的意义：
   + 离线评估无法完全消除模型过拟合的影响。
   + 离线评估无法完全还原线上的工程环境。线上可能面临着延迟，数据丢失，标签数据缺失
   + 线上系统的某些商业指标在离线环境下不能计算
   
2. 如何进行线上的ab测试：
   + 对用户进行分桶，分成实验组和对照组，对实验组投放新模型，对对照组投放旧模型。分桶过程中要注意样本的独立性和无偏性
   + 注意用户特性，实验组和对照组除了模型不一样之外，其余用户特征应该是一样的。
   
3. 什么是过拟合吗？如何解决过拟合？如何判断过拟合？
   + 定义：对训练数据的拟合过当
   + 判断：在训练集上表现很好，在测试集，验证集上表现较差
   + 降低过拟合：
     + 获取更多的训练数据
     + 降低模型复杂度：减少网络层数，神经元个数等
     + 对参数使用正则化方法，即也要降低参数的损失
     + 集成学习方法：多个模型拼接到一起，降低单个模型的过拟合
   + 降低欠拟合：
     + 添加新特征（特征工程）：使用因子分解机，梯度提升决策树，deep-crossing
     + 增加模型复杂度
     + 减小正则化系数
   
4. softmax
   + 定义：能够将各输出节点的输出值映射到[0,1]，并且约束映射后的各节点输出值变成一个概率分布，输出值的和为1的函数
   + 对于某种意义上的hard-max，也就是直观的取出数组中的最大值。而在实际处理过程中，只找到一个最大值是不够的，我们更期望得到文章对于每个可能的文本类别的概率值（可以认为是某一个文章属于任意类别的可信度）
   + 含义：不再唯一确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值
   + 引入指数函数的优点：
     + x轴上的一个小变化，会带来y轴上较大变化
     + 常使用梯度下降进行参数更新，指数在求导比较方便
   + 引入指数的缺点：
     + 当数值很大时，可能会产生溢出（优化方式：在softmax公式中，将每一个输出值减去输出值中的最大值）
   + 根据softmax推导出的损失函数，与传统的损失函数有什么区别？
     + 对于softmax推导出的损失函数，只针对正确类别对应的输出节点，将这个位置的Softmax值最大化
     + 对于传统的损失函数，直接衡量真实分布和实际输出的分布之间的距离。
   
5. 调参

   + 寻找合适的学习率
     + batch越大，学习率越大
     + 在显存足够大的条件下，最好采用较大的 batch_size 进行训练，找到合适的学习率，然后加快收敛
     + 较大的 batch_size 可以避免 batch_normalization 一些问题

   + 权重初始化

     + 一般使用预训练的模型
     + kaiming_normal
     + Xavier_normal

   + dropout

     + 类似于 bagging ensemble 减小 variance，通过投票来减少可变性
     + 一般适用于全连接层，卷积层不太需要

   + 多模型融合

     + 同样的参数，不同的初始化方式

     + 不同的参数，通过cross-validation，选取最好的几组

     + 同样的参数，模型训练的不同阶段，即不同迭代次数的模型

     + 不同的模型，进行线性融合，例如RNN和传统模型

     + 提高性能和鲁棒性大法：

       + probs融合，投票法

       1. model1 probs + model2 probs + model3 probs ==> final label
       2. model1 label , model2 label , model3 label ==> voting ==> final label
       3. model1_1 probs + ... + model1_n probs ==> mode1 label, 同理 model2 label与model3 label方式与1相同 ==> voting ==> final label
   
6. AUC

   + 混淆矩阵
     + True Postive（TP，正样本且预测对），False Postive（负样本且预测错），







### 数学相关：

1. 极大似然估计（Maximum likelihood estimation）：

   + 用已知的样本信息，反推导致这些样本结果出现的最大概率的模型参数
   + <font color=red>看上去有点像一个无监督学习，一直观测状态，但是未知隐藏状态。总结来说，这个极大似然估计很像隐马尔可夫模型能够解决的学习问题，也就是无监督学习问题。但是极大似然估计是不存在隐变量的，所以也不能解决转移概率的问题。只能说他们之间只是思想相似</font>

   + 举例来说，通过所有可观测到`x`和`f(x)`的值，能够求出正态分布中的参数 $\mu$ 和 $\sigma$ ，那这个模型中所有的信息都可以获得
   + 假设：所有的采样都是独立同分布的
   + 对似然函数 $p(x|\theta)$ 的理解
     + 如果 $\theta$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 $x$ ，其出现概率是多少。
     + 如果 $x$ 是已知确定的，$\theta$ 是变量，这个函数叫做似然函数 (likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。
   + 参数的选择有无数种，具体要选哪一种，是要让这个样本出现的可能性最大（尽可能接近我们预测的值），这个最大可能性的结果就对应我们要找的那一套参数。
2. 介绍一下各种loss function

   + 对于分类问题：
     + 0-1损失：对于大于0的位置不惩罚，有不可导点
     + hinge损失：对于大于1的位置不惩罚，有不可导点
     + logistic损失：对于所有样本都有惩罚，处处可导，对异常值比较敏感
     + 交叉熵损失函数：针对预测值在[-1, 1]中，处处可导
   + 对于回归问题：
     + 平方损失：对于较远点惩罚力度大，比较敏感，
     + 绝对损失：预测值与真实值相等处不可导，对异常点比较鲁棒
     + Huber损失：在较小值处使用平方损失，在较大值处使用绝对损失
3. 常见的激活函数有哪些？都有什么特点？

   + Sigmoid：$f(z) = \frac {1}{1+exp(-z)}$，导数：$f(z)(1-f(z))$，
     + 可能出现梯度消失，在z很大/很小时
   + Tanh：$f(z) = tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$，导数：$1-f(z)^2$，
     + 可能出现梯度消失，在z很大/很小时
   + ReLU：$f(z) = max(0, z)$，导数：  $f(n) = \begin{cases} 1, & z>0 \\ 0, & z<=0 \end{cases}$
     + 优点：
       + 只要一个阈值就可以得到激活值，而不必像sigmoid和tanh那样计算指数
       + 非饱和性可以有效的解决梯度消失的问题，提供相对宽的激活边界
       + 单侧抑制提供了网络的稀疏表达能力
     + 局限性：
       + 负梯度在经过ReLU后被置为0，该神经元的梯度永远为0，不对任何数据产生响应
       + Leaky ReLU $f(x) = \begin{cases} z, z>0; \\ az, z <=0; \end{cases}$。一般a很小，这样既实现了单侧抑制，有保留了部分负梯度信息；但是a的值需要多次实验已确定
       + Parametric ReLU 将其中的a作为一个可学习参数，加入到模型中进行训练
       + 或者令a满足某种分布的随机采样
4. 为什么需要激活函数

   + 可能存在线性不可分的问题，需要非线性变换对数据的分布进行重新映射
   + 对于深度神经网络，添加激活函数能够避免多层网络等效于单层线性函数，使其能够获得更强大的学习于拟合能力
5. 如果数据不充足，或者不平均，要怎么解决？从数据增强和建模的方面解释（<font color=red> 数据不平衡问题 </font>）
   + 最简单的方式为随机采样
     + 随机过采样：从少数类样本集中随机重复抽取样本，有放回
     + 随机欠采样：从多数类样本集中随机选取较少的样本，有无放回均可
     + 过采样对少数样本进行多次复制，容易产生过拟合；欠采样可能会丢弃一些重要的信息，模型又可能只学到了整体特征的一部分
     + 可以使用==SMOTE==算法优化随机过采样。对少数类样本集中的每个样本x，从它在样本集中的K近邻中随机选取N个样本，在这样本x与这N个样本的连线上随机选取一个点作为新合成的样本（线性插值）。能够降低过拟合
     + 可以采用informed undersampling 来解决欠拟合的问题。常见的方法有Easy Ensemble，Balance Cascade，
   + 基于算法的优化方式
     + 改变模型训练时的目标函数来矫正这种不平衡性
     + 当样本数量及其不平衡时，也可以将问题转化为单类学习，异常检测等
6. 数据扩充的方法
   + 词汇替换：从句子中随机抽取一个或者多个单词，使用同义词进行替换。中文同义词可以训练一个word2vec来查找；英文同义词可以使用WordNet来查找同义词
   + 词embedding替换：直接计算词embedding的距离，然后用距离最近的N个词的embedding进行替换
   + MLM（Masked Language Model）：BERT、ROBERTA和ALBERT等模型已经在大量的文本上进行了训练，我们可以使用其进行文本的扩充，随机Mask个文本中的某个单词，然后使用模型对其进行预测。
   + 机器翻译：该策略的思路是先将原先的文本转化为其它回家的语义，然后再转化回来得到新的重新翻译之后的文本。该策略经常在一些小数据集的文本上被使用。
   + 文本表面转化：该转换的策略也非常简单，例如下面的就是She's ——> She has.
   + 错误注入拼写：我们将拼写错误添加到句子中的一些随机词中。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
   + QWERTY键盘错误注入：QWERTY键盘错误注入是直接模拟键盘输入出错的策略
   + 空格注入：这种思想是使用占位符标记替换随机单词。论文使用“_”作为占位符标记。
   + 句子Shuffliing：句子Shuffliing的策略就是对整个句子进行shuffle，切换句子的位置。
   + 交叉扩充：例如在tweet的文本情感中，一个tweet被分成两半，并且具有相同标签（正/负）的两个随机tweet被随机分开。假设是，即使结果不符合语法和语义，新的文本仍然会保留相同的情感。
7. 正则化

   + 参考资料：https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/

   + 为什么使用正则化：

     + 为了防止过拟合，增强泛化能力

   + 什么是正则化：从线性模型角度

     + 给训练的目标函数上加上一些规则，减小其过拟合情况的方法

     + 一般正则项：$\frac {\lambda}{2}\sum_{j=1}^M \vert\omega_j\vert^q$

       + M 表示模型的阶次，也就是数据的维度，当 M=2 时，也就是二位平面内的点
       + q = 2 就是二次正则项
       + ![q次正则项](/Users/lixuanhong/Desktop/Materials/q次正则项.png)

       + <font color=red>正则化能够防止过拟合的原因</font>：直观理解为目标函数的最小值，也就是求误差函数与正则项函数的和的最小值，这个值通常是两个曲面相交的地方
       + <font color=red>二次正则项的优势</font>：处处可导，限制模型复杂度，即 `w` 中的 `M` 的大小，`M`越小模型越简单
       + <font color=red>一次正则项的优势</font>：$w^*$ 的位置正好0（交点位置为0），相比二次正则项，维度降低更具体，更稀疏。但是一次正则项有拐点，不是处处可微的，但是可以通过一些技巧补充或避免这个问题

   + 什么是正则化：从神经网络的角度

     + 这里的 `M` 表示为神经网络中隐藏层中的神经元的数量
     + 数量越大，模型越复杂，过拟合的可能性越大
     + 未来保持神经网络的一致性（即输出的值不能被尺缩变换，或平移变换），在线性模型中的**加入正则项**无法奏效；**所以我们只能通过建立验证集（Validation Set），拉网搜索来确定`M`的取值（迭代停止的时间），又称为【提前停止】**

   + 范数：

     + **范数是具有“长度”概念的函数**，用于衡量一个**矢量的大小**（测量矢量的测度）
     + 如何表达“长度”这个概念是不同的，也就对应了不同的**范数**，本质上说，还是**观察问题的方式和角度不同**
     + 范数分为**向量范数**（二维坐标系）和**矩阵范数**（多维空间，一般化表达）
     + 针对向量范数：**0-范数：向量中非零元素的数量；1-范数：向量的元素的绝对值之和；2-范数：是通常意义上的模（向量元素的平方再开方，欧式距离）**
8. 优化器
   + 批量梯度下降
     + 优点：能够获得准确的梯度
     + 缺点：载入整个训练集进行计算，时间花费和内存开销都很大
   + 随机梯度下降
     + 优点：放弃准确的梯度，通过随机采样来计算梯度
     + 缺点：接受的信息量有限，对梯度的估计有偏差，使得目标函数收敛不稳定，甚至不收敛；容易在山谷中产生左右碰壁，在鞍点处梯度停留在水平位置，不能继续向下找到最优（山谷震荡，鞍点停滞）。
   + momentum（基于历史信息，保持惯性）
     + 优点：当前更新步伐除了学习率乘以当前梯度，还和上一步的更新步伐有关；可以理解为物理中求当前速度需要考虑到前一时刻的速度和当前的加速度；上一步更新步伐中的系数是衰减系数；带惯性的优化方法收敛速度更快，收敛曲线更稳定。
   + AdaGrad（对环境的感知，衡量梯度的稀疏性）
     + 文本建模中，词频比较小的对应参数更新频率低。但是我们希望更新频率低的参数可以拥有较大的更新幅度，而更新频率高的参数的幅度可以减小
     + 优点：历史梯度平方和，针对每个特定维度来更新学习率。衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。随着时间的推移，学习率越来越小，保证算法收敛。
   + Adam（惯性保持+环境感知）
     + 优点：记录一阶矩，过往梯度与当前梯度的平均，体现惯性保持；记录二阶矩，过往梯度的平方与当前梯度的平方的平均，体现环境感知。时间距离越近的过往梯度影响越大
     + 一阶矩：相当于估计当下梯度的期望，因为是随意采样的结果，因此更关注其在统计意义上的期望
     + 二阶矩：相当于估计当下梯度的平方的期望，这里与AdaGrad不同，不是累加，而是期望
     + 物理意义：
       + 一阶矩大，且二阶矩大，梯度大且稳定，前进方向明确
       + 一阶矩趋于零，且二阶矩大，梯度不稳定，可能遇到峡谷，容易引起反弹
       + 一阶矩，二阶矩都趋于零，可能达到局部最低，或平缓位置
