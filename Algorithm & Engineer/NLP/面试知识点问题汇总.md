如何准备场景题：

1. 特定，垂直场景下的特征提取（特征工程）
   + 那些特征可以提取，如何提取



问题汇总：

2. 神经网络为什么会产生梯度消失现象？
3. 挑一种激活函数推导梯度下降的过程？
4. Attention都有哪些变种？
6. 如何将attention机制引入关系抽取的？
   + 见关系抽取模型
7. 为什么要引入attention？
   + 见关系抽取模型
7. 如何部署？
8. t-检验
9. GBDT：梯度提升决策树
10. kmeans公式推导，证明kmeans极值点数学表达
11. MLE(maximum likelihood estimation) 和MSE (mean square error)的关系，对高斯函数求个log就出来了
12. xgb和gbdt的区别 (几乎必问的题目，提前准备一下，说的要有条理，有哪些[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)优化，哪些工程实现优化，可以适当扩展提一下lgb)
13. || x ||范式是什么意思？
14. 正则化是什么意思？有什么用？
    + 在反向传播时存在于损失函数中，能够减小权重的幅度，防止过拟合
15. 调参流程
16. 多任务学习
17. 如果多任务学习输出的y之间有相互制约的关系，应该使用CRF进行处理
18. 多模态学习
19. 还需要再看一下GNN相关的内容
20. 如果数据不充足，或者不平均，要怎么解决？从数据增强和建模的方面解释
21. 集成学习两种方式对比
22. 左轮手枪可以装6颗子弹，两颗子弹相邻装在一起，现在别人打了一枪没死，你选择接着打还是随机转一圈再打。
23. 艾滋病患病概率1/100，误诊概率1/100，一个人诊断患病，则实际患病的概率
    + 贝叶斯公式，假设真实患病的概率为A，确诊患病的概率为B，实际要求的是$P(A|B)$，使用贝叶斯公式：$P(A|B) = \frac {P(A)P(B|A)}{P(A)P(B|A) + P(\bar A)P(B|\bar A)}$。误诊的概率其实是$P(B|\bar A), P(\bar B|A)$，那么诊断正确的概率就是$P(\bar B|\bar A), P(B|A)$，患病的概率就是 $P(A)$，结果为0.5
24. 生了俩孩子，有一个女的，另一个是男的概率
25. 进程和线程
26. 模型蒸馏
27. 特征工程的常见手段（去噪等）
28. 讲一下roc曲线和AUC值；真阳性和假阳性怎么算的？
29. 讲一下改进的tf-idf
30. 显存不够怎么解决？
31. 100个python的问题
32. bert 字向量的embedding怎么训练得到的
33. layer normlization 和 batch normlization
34. 为什么Adam有时候可能没那么好，怎么解决
35. 词向量负采样的细节，数学原理
36. 问大规模的用户导致维度爆炸怎么处理
37. 了解什么评估指标。说了recall，precision，f1-score以及AUC，手动实现AUC
38. 英语流利说面试
    + 使用关键词提取，和人工标注关键词的方法，使用起来各有什么优缺点
    + 如何评估实体识别和关系抽取的结果（除了常见的准确率评估外）
      + ROC，AUC，PR曲线
    + CRF为什么能做到全局考虑，什么原理和模型？计算路径分数和，
    + 负采样有什么作用，负采样的方法
39. 叮咚买菜
    + 如何评估实体识别和关系抽取的结果（除了常见的准确率评估外）
    + 哪两种mask的方式
    + 手写multihead-attention的算法（Q，K，V）
    + 使用过的模型的改进空间：
      + 关系抽取的句子级别attention中的不同batch-size，预测结果差别较大
      + 两个假设，两类实体，一种关系，可以使用细粒度情感分析的方式进行优化
      + 抽取完成后，将这个关系同样应用在同类型的其他实体之间（增量）
    + adam优化器的优劣
    + 为什么选用使用BiGRU而不使用BiRNN（这个问题感觉是在问GRU比RNN的优势，回答没尝试就太没水平了）
40. 英语流利说
    + 其他能够解决图谱补全的方法（基础模型，原理及与RGCN的区别，模型选型）
    + RGCN的模型结构，公式及评估方式（评估方式还是要看一下论文）
    + 字典树
41. 云从
    + 最长回文子串
    + 最长回文子序列
    + 需要看一下实体识别，关系抽取的评估方式，对于识别或抽取不准的情况应该如何处理
    + 实体类型，关系类型，各自数量有多少？
    + 外部找到的结构化数据如何与本地产生的数据进行融合
    + 问答结果中识别到的关键词如果不能与词表中的任何词进行映射怎么办？
      + 用于输入问题中关键词表达模糊，如何做<font color=red>实体对齐</font>
42. 比心陪练
    + 垃圾公司，毫无收获
43. 滴滴一面：
    + 实体识别结果的评价方式（面试官说使用精确率，召回率可能不太准确，那就使用f1）
    + 实体识别任务中，训练数据强规则生成，存在不合理性
    + 实体对齐的问题
    + QQ类问答不是很了解传统的召回排序思路
    + KBQA的常规做法
44. 拼多多
    + 项目方面：
      + 知识图谱：实体对齐，实体消歧
      + 实体识别：基于词表的数据标注，训练出来的基本收敛于词表，如何发现新词
      + 关系抽取：为什么使用GRU，为什么不都使用LSTM，为什么要区别对待
      + RGCN：预测潜在关系的三元组向量的乘积，复杂度比较高，使用向量求解，首先计算一个节点和关系的向量，使用fasttext来计算另一个节点的向量
    + bert
      + 三个embedding向量：输入词向量（token embedding）怎么得到
        + bert输入是subword，bert自己自带一个词表，输入subword在词表中的id，然后将bert输出和你自己模型的随机初始化词向量拼接，
      + 为什么bert和transformer的position embeddig方式不一样
        + 可能sin编码方式效果是不错的，正好被试出来了；但是随机初始化去学习，应该是能够获得更好的特征表达，毕竟是在大量通用语料上进行学习的
45. 云从科技
    + CRF的两个假设
    + LSTM和GRU的公式
    + 使用快排找数组中第k大的值
    + 第k大的值是不是都可以用on复杂度实现
46. 天眼查
    + attention中为什么要除以根号dk
      + dk太大，导致值大的预测概率接近于1，
      + 防止对softmax求导时梯度太小
47. 滴滴二面
    + transformer 中为什么使用 layer normlization 而不是 batch normlization
    + 没有transformer相关项目经验
    + CRF的维特比算法解码（算法方面） <font color=red> done </font>
    + 文本生成不为什么不能使用维特比算法（在所有词表上（全局）计算的，量级比较大）<font color=red> done </font>
    + 集束搜索
48. 58同城
    + lstm的参数量，已知 hidden_size 和输入数据的 max_len，embedding_size 求它的参数量
      + 参数量：`4*(hidden_size * (embedding_size + hidden_size) + hidden_size)`
        + 4表示共有4组公式
        + `embedding_size + hidden_size`：表示 `x` 与上一个时刻 `h` 的参数量，表示这是输入数据
        + `hidden_size *` ：表示为输出，也就是这一层的参数量
        + `+ hidden_size`：表示这一层的bias偏置
    + GCN中度矩阵的作用
      + 由于度大的特征比较大，度小的特征比较小，所以需要做归一化
      + 辅助计算归一化，常用方法为几何归一化，在拉普拉斯矩阵两端同时乘以度矩阵的负1/2次方
    + layer norm 是否有参数
      + 有参数的，要训练的
    + CRF转移矩阵的维度，损失函数是哪个
      + 转移矩阵维度：所有标签的个数（7个实体，14个标签，o，\<UKN>，所以是16*16）
      + 损失函数：真实路径分数比上所有路径分数，最大（加入log计算并加入负号，使其最小）
49. 金山二面：
    + 搜索旋转排序数组
    + 实体链接
50. 滴滴三面
    + 如果实体识别的训练数据是拿固定词表进行标注的，那么如何评估模型的效果
      + 既然已经有词表了，为什么还要用模型，模型能为词表补充的情况是怎么样的？多少的文本能够增多少的实体词（这里还是应该去做一下）
    + bert是如何来做序列标注，即实体识别的，过程和步骤，需要哪些东西 <font color=red> done </font>
51. 陌陌一面
    + layer norm 有什么作用
      + 特征缩放
      + 把输入转化成均值为0，方差为1的数据，在送入激活函数之前进行，希望数据不要落在激活函数的饱和区
    + 指代消岐，实体对齐具体做法
    + 训练模型上的trick
      + 见炼丹.md
52. 美篇
    + 如何评估智能问答的效果
      + 人工创建测试数据，针对问题选择正负样本问题，计算top@n，即预测结果最高的几个结果中证样本所占的比例
      + 响应时间：
    + 分层softmax，哈夫曼树详解 <font color=red> 完成 </font>
    + 词向量的演进过程
53. 陌陌二面
    + 鸡蛋掉落
    + 重点关注一下什么叫最少试验次数
54. 欢聚集团
    + 残差网络和layer norm的两个实现方式
    + dropout在训练和测试阶段有什么不同，集成？
      + 测试阶段：不关闭dropout，得到的就是一个随机的网络，给定x，得到的预测值y是服从一定的分布的。
      + 单次预测就是从这个分布中采样一次。多次采样就可以得到y的均值，也就是最终的预测值。
      + 假设：平均参数（关闭dropout）得到的模型的预测值近似等于上面说的多次采样的平均值。<font color=red>所以这个时候就有了训练开dropout，预测时关闭的做法。</font>
    + 逆向文本最大匹配+前缀树
55. Opera
    + bert和GPT的区别





整体优化方法：

+ 使用词典来标注实体识别训练语料的不足性（未登录词）
  + 使用 AutoNER 对实体词进行 fuzzy-lstm-crf 和 Tie or Break 标注（可参考知乎中相关文章及代码实现）
  + **fuzzy-lstm-crf**
    + 假如能够知道一个实体：但是不知道实体类型
    + 将这个实体标注为每一种实体类型，还是对其进行BIOSE标注
    + 传统的CRF仅最大化真实的字标签序列出现的概率，而考虑到现在有多个可能的标签序列，因此改为最大化这些标签序列的概率之和
  + Tie or Break
    + Fuzzy-lstm-crf 不能对远程监督带来的噪声问题进行处理
    + 提出了Tie or Break方案，Tie表示当前词和上一个词在同一个实体内，Break表示当前词和上一个词不再同一个实体内。
    + 字标签与实体类型分开预测
    + 一个句子有如下标注，其中B表示Break，T表示Tie，U表示Unknown，N表示None
    + 在预测 span 标签序列时，只预测是tie还是break，在计算损失时，忽略真实为 Unknown 的 token。而在计算type损失的时候，包括了序列中所有的token。
    + 在span和type的预测中，没有使用CRF。
  + 在全局语料上进行新词发现任务，对词典进行补充
  + 训练实体识别模型，保召回率
+ 实体链接
  + 比对待对齐mention（及其上下文）与相关mention（及其上下文）之间的相似度，语义选择为相似度高的
  + https://github.com/AlexYangLi/ccks2019_el
  + 指代消岐，实体对齐的任务都可以采用
  + 实践：ccks2019_el 数据，已下载到桌面
+ 问答：
  + 当前基于word2vec的问答模型如何评价
    + top@k命中方法：标准答案要出现在top@1，top@3，top@5
  + 召回：基于BM25的简单召回和语义向量的双路召回
    + 基于语义向量的召回：Embedding-based Retrieval in Facebook Search
    + https://zhuanlan.zhihu.com/p/349993294
  + 排序：<font color=red>排序方法还没确定</font> 
  + 看一下句向量的产生方式
  + 实践：拿到已有数据进行使用（蚂蚁金服的数据）



### embedding相关

1. embedding的方法？

+ 词袋模型：
  + one-hot
  + tf-idf
  + TextRank
  + fasttext
  + <font color=red>问题</font>：维度灾难，语义鸿沟
+ 主题模型
  + 分布式表示
  + LSA
  + LDA
  + <font color=red>矩阵分解问题</font>：利用全局语料特征，但SVD求解计算复杂度大；
+ 基于词向量的静态表示
  + 分布式表示
  + word2vec
    + skip-gram：根据中间词预测上下文
    + CBOW：根据上下文预测中间词
      + 参数个数：包含两个参数矩阵，第一个参数矩阵维度为[V, N]，用来将多个维度为V的词向量转化为多个维度为N的向量，这些向量通过加权平均后（这里可以获取到预测词的词向量），与第二个维度为[N, V]的参数矩阵相乘，得到维度为V的预测词
    + 词嵌入可将词的特征映射到较低的维度，使用模型参数更少，训练更快。
    + 将onehot编码，通过与参数矩阵乘积，转换成embedding
    + 问题：优化效率高，但是其是基于局部语料的
    + <font color=red>对比 NNLM</font>：
      + 本质都可以看作是语言模型；
      + 词向量是NNLM的产物，而word2vec专注于词向量本身
      + 预测当前词时，是上下文词降维后直接相加的，而不是拼接，也没有隐层
      + 使用 hierarchical softmax 和 negative sampling 进行优化，不再遍历整个词表
        + hierarchical softmax：生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小
        + negative sampling：降低每一个词负样本的预测概率，加快运算；
    + <font color=red>对比 fastText</font>
      + 都可以无监督学习词向量， fastText训练词向量时会考虑subword
      +  fastText可以进行有监督学习，进行文本分类
        + 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径
        + 引入N-gram，考虑词序特征
        + 引入subword来处理长词，处理未登陆词问题
    + <font color=red>对比 Glove</font>
      + word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于<font color=red>全局语料</font>的，可见glove需要<font color=red>事先统计共现概率</font>；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。
      + word2vec是无监督学习，实际上glove还是有label的，即共现次数$log(X_{ij})$
      + word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。
      + **glove可以被看作是更换了目标函数和权重函数的全局word2vec**
  + GloVe
    + 基于全局语料库，并结合上下文语境构建词向量，结合了LSA和word2vec的优点
    + 构建过程
      + 根据语料库构建一个共现矩阵，矩阵中的每一个元素 $X_{ij}$ 代表单词 ![[公式]](https://www.zhihu.com/equation?tex=i) 和上下文单词 ![[公式]](https://www.zhihu.com/equation?tex=j) 在特定大小的上下文窗口内共同出现的次数。
      + 构建词向量和共现矩阵之间的近似关系，目标函数的基本形式就是平方差损失函数，在其基础上添加了一个权重函数
    + 训练过程：
      + 虽然不需要人工标注，但还是有标签的，就是共现次数$log(X_{ij})$
      + 向量 $\omega$，$\bar \omega$ 为学习参数，本质上与监督学习的训练方法一样，采用了AdaGrad的梯度下降算法，对矩阵 X 中的所有非零元素进行随机采样，学习曲率（learning rate)设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。
      + 最终学习得到的是两个词向量是 $\bar \omega$ 和  $\omega$，因为 X 是对称的（symmetric），所以从原理上讲 $\bar \omega$ 和 $\omega$ ，是也是对称的，<font color=red>他们唯一的区别是初始化的值不一样，而导致最终的值不一样 </font>。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和 $\omega + \bar \omega$ 作为最终的vector（<font color=red>两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性</font>)。
    + 可以进行词向量的动态表征
    + 对比<font color=red> LSA </font>
      + LSA 基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高；glove可看作是对LSA一种优化的高效矩阵分解算法，采用Adagrad对最小平方损失进行优化；
+ 基于词向量的动态表示：
  + 解决静态表示不能解决的<font color=red>一词多义</font>的问题

  + elmo

    + 相比word2vec
      + 基于上下文能够建模动态词向量
      + 网络层数比较深
      + 能解决一词多义的情况，并且词性也能对应起来
    + 模型：
      + 两个单向（方向相反）单层LSTM，<font color=red>建模预训练语言模型得到词向量</font>
      + 第一层输入为词特征，第二层LSTM为句法特证，第三层LSTM为语义特征，使用时将三个embedding加权求和，代表一个词的embedding。其中这个词的embedding可以作为初始化权重，在下游任务中进行学习
    + 使用：
      + 对于具体的任务，首先应该使用语料通过预训练模型，获取词向量，即词特征，然后再使用词特征去接后面的网络进行下游任务
  
  + GPT

    + 预训练
      + 建模语言模型获取句子向量，预训练模型 + FineTune
      + <font color=red>特征抽取器使用的是Transformer Decoder，建模单向语言模型</font>
      + Attention 只能拿前文的词去预测当前词
    + FineTune
      + 这个模型拿过来嵌入到下游任务的网络中
  
  + bert
  
    + https://www.jianshu.com/p/4dbdb5ab959b
    + 全称：Bidirectional Encoder Representation from Transformers
    + 核心：**Transformer Encoder**
    + **为什么bert采取的是双向Transformer Encoder，而不叫decoder？**：
    
      + BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获得要预测的信息的。
    + **bert不使用拼接两个反向的transformer decoder的原因**
  
      + BERT 的作者认为，这种拼接式的bi-directional 仍然不能完整地理解整个语句的语义。更好的办法是用上下文全向来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。
    + <font color=red>bert为什么要采取Marked LM，而不直接应用Transformer Encoder？</font>
    
      + 深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。
      + 为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)。
    + <font color=red>bert为什么并不总是用实际的[MASK]token替换被“masked”的词汇？</font>
    + <font color=red>预训练和fine-tuning之间不匹配，因为在fine-tuning期间从未看到被[MASK]的token。</font>为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。80％的情况用[MASK]标记替换单词；10％的情况用一个随机的单词替换该单词；10％的情况保持单词不变。==原因1：==<font color=red>这样做的目的是将表示偏向于实际观察到的单词（后续微调时输入的数据并不包含mask）</font>。==原因2：==<font color=red>Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示，同时也赋予了bert一定的纠错能力</font>。因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。
    + <font color=red>每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。</font>团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。
    + next sentence prediction
      + 给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后
    
  + 三者区别：
  
    + | 模型        | elmo                           | GPT                                                       | BERT                            |
      | ----------- | ------------------------------ | --------------------------------------------------------- | ------------------------------- |
      | 特征提取器  | LSTM                           | Transformer                                               | Transformer                     |
      | 层数        | 1层静态词向量+2层LSTM          | 多层                                                      | 多层                            |
      | 语言模型    | 双向（两个方向相反的单向拼接） | 单向                                                      | 双向                            |
      | transformer | 不采用transformer              | 采用Transformer decoder部分，即从左到右，看到的句子不完整 | 采用encoder部分，看到的句子完整 |
      
    + bert 输出：<font color=red>输出句子向量的语义表征信息</font>
  
      + last_hidden_state：shape是(batch_size, sequence_length, hidden_size)，hidden_size=768,它是模型最后一层输出的隐藏状态，序列标注通常用。
      + pooler_output：shape是(batch_size, hidden_size)，是last_hidden_state[0]。这是序列的第一个token(classification token)的最后一层的隐藏状态[CLS]的向量表征（基础版分类用的这一层）
      + hidden_states：这是输出的一个可选项，如果输出，需要指定 config.output_hidden_states = True, 它也是一个元组，它的第一个元素是embedding，其余元素是12层transformer各层的输出，每个元素的形状是(batch_size, sequence_length, hidden_size),共计13个元素，最后一个元素是last_hidden_state。
      + attentions：这也是输出的一个可选项，如果输出，需要指定config.output_attentions=True,它也是一个元组，它的元素是每一层的注意力权重，用于计算self-attention heads的加权平均值
  
    + 缺点：
  
      + MLM直接对<font color=red>单个token进行随机mask，丢失了短语和实体信息</font>，这一点对中文尤其明显，百度ERNIE对其进行了改进
      + MLM仅预测被mask的token，<font color=red>其他token没有参与到预测中，导致MLM训练效率偏低，训练时间过长。</font>ELECTRA围绕这一点进行了改进。
      + <font color=red>NSP任务的负样本构造过于简单，导致模型不能充分训练，NSP对下游任务的作用比MLM要小。</font>SpanBERT、ALBERT、Roberta均提到了这一点，并进行了相关改进，或者干脆弃用NSP。
  
  + ALBERT:  A Lite BERT
  
    + 减小参数量：
      + <font color=red>嵌入参数化进行因式分解</font>。大的词汇嵌入矩阵分解为两个小的矩阵，将隐藏层的大小与嵌入层的分离开。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。不再将 one-hot 向量直接映射到大小为 H 的隐藏空间，先映射到一个低维词嵌入空间 E，然后再映射到隐藏空间。通过这种分解，研究者可以将词嵌入参数从 O(V × H) 降低到 O(V × E + E × H)
      + <font color=red>跨层参数共享</font>。这一技术可以避免参数量随着网络深度的增加而增加。
    + 训练任务
      + <font color=red>Sentence-order prediction</font> (SOP) 来取代NSP。其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，<font color=red>模型学习到的就更多是句子间的连贯性</font>。用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。
  
  + Robert 
  
    + 改进优化函数
    + 采用动态掩码，每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。而BERT预处理好一个掩码模型，后面直接调用
    + 使用更大的数据集，更大的batch
    + 使用BPE（Byte-Pair Encoding）来处理文本，是字符级和词级别表征的混合，用更大的 byte 级别 BPE 词汇表，且没有对输入作任何额外的预处理或分词。
    + 去掉了NSP任务，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES）
  
  + ERNIE
  
    + 利用短语和实体级别的mask方式，来融合外部知识
    + 添加更多优质中文语料
    + DLM。对Dialog的角色，进行了Dialog embedding，从而加强模型在Dialog上的效果



### 通用模型相关

CNN：

+ 稀疏交互（局部连接）
  + 意义：文本数据可能包含局部特征，可以选择先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征
  + 优势：优化过程中的时间复杂度将会减小几个量级；一定程度上避免过拟合
+ 参数共享（卷积运算的固有属性）
  + 意义：使得卷基层具有平移等变性。可以简单理解为同一种特征不论出现在任意位置都应该能够被识别。神经网络的输出对于平移变换来说应该是等变的
  + 优势：降低模型的存储需求
+ 池化
  + 本质：降采样
  + 平均池化：
    + 作用：能够抑制由于邻域大小受限造成估计值方差增大的现象
  + 最大池化
    + 作用：能够抑制网络参数误差造成估计均值偏移的现象
  + 相邻重叠区域池化：
    + 采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠的区域
  + 空间金字塔池化
    + 考虑多尺度的信息，同时计算1\*1，2\*2，4\*4的矩阵的池化并将结果拼接在一起作为下一层的输入
  + 作用
    + 显著降低参数量， 还能够保持对平移，伸缩，旋转操作的不变性
    + 平移不变性：输出结果对输入的小量平移基本保持不变
    + 伸缩不变性：经过尺度变换后，大概率保证最大池化的值不变。因为神经元感受的是邻域输入最大值，而非某一个确定值
    + 旋转不变性：多个过滤器分别学习不同方向的输入数据，即使输入数据方向改变了，也不会影响池化响应的结果
+ 用于文本分类任务
  + 输入层
    + 词向量可以提前在其他语料中训练好，优势是能够获得很多先验的知识
    + 或者作为参数在CNN中进行训练，优势是能够获取与任务相关的特征
    + 输入层可以同时采用两种方式，一种使用先验训练好的词向量，在训练过程中不再更新；另一种将训练好的词向量拿来初始化，跟随模型一起训练
  + 卷积层
    + 定义大小不同的滑动窗口进行卷积操作，在窗口上得到的卷积结果进行拼接
    + 每一次卷积相当于一次特征提取
  + 池化层
  + 全连接
+ 深度残差网络
  + 网络层数加深带来的问题
    + 优化函数越来越陷入局部最优解
    + 梯度消失的问题更加严重，梯度在反向传播时衰减严重
  + 主要解决：
    + 梯度消失问题
    + 经过两层得到的变换是F(x)，加上离输入近的神经网络层（因为其距离输出层较远，训练比较难，这里直接进行短接）x，得到最终的输出就是H(x) = F(x) + x
    + F(x)只需要拟合输入x与目标输出H(x)的残差--H(x) - x
    + 如果某一层的输出已经很好的拟合了期望结果，那么多加入一层（<font color=red>应该是加到已有层的前面</font>）不会使得模型变得更差，因为多加入的这层的输出将直接被短接到两层之后，相当于直接学习了一个恒等映射，而跳过的两层（即上面的F(x)）只需要拟合上层输出（上面的x）和目标之间（即上面的H(x)）的残差



### 实体识别模型

1. 为什么实体识别模型要使用BiLSTM+CRF，CRF起到的作用是什么？

+ BiLSTM本身已经能够预测文本序列与标签之间的关系，而不能预测标签与标签之间的关系，标签之间的相互关系就是CRF中的转移矩阵；CRF中会通过转移矩阵计算每个序列路径的打分情况，将得分最高的那条路径序列作为最终的结果；CRF能够在BiLSTM的预测结果的基础上，根据句子的约束条件，修正预测结果，保留最佳路径。

2. 简述转移矩阵和状态矩阵

+ 转移矩阵：由上一个预测的实体类别到下一个实体类别的转移分数组成，在训练模型时，可以对这个转移矩阵进行随机初始化，这些分数会在训练中被更新，也就是说CRF可以自己学习到
+ 状态矩阵：由单词的位置索引和实体类型索引组合，用来记录某个单词被预测为某个实体类型的分值，这些分值来自于BiLSTM层的输出

3. 为什么要使用BiLSTM，相比LSTM有什么优点，有什么共同点？

+ LSTM在句子建模时的优点：通过词向量相加（或均值）得到的句向量不能表示词之间的顺序，但是<font color=red>LSTM能够更好捕捉到较长距离的依赖关系</font>，可以通过训练过程学习到应该记忆哪些信息和忘记那些信息
+ 优点：由前向LSTM与后向LSTM组合而成，可以更好的捕捉双向的语义关系
+ 共同点：两者在自然语言处理任务中都常被用来建模上下文信息

+ 句子编码方式依赖于对向量拼接，不同的拼接结果对应着不同的目的。
  + 如果需要取出对于每个词的向量，那么需要将每个位置，不同方向的LSTM的输出结果进行拼接（concat），如下图所示：
  + 三个词的编码结果分别为，$[h_{L0}, h_{L1}, h_{L2}]$ 和 $[h_{R2}, h_{R1}, h_{R0}]$ ，那么最终的编码方式为 $[h_0, h_1, h_2]$，其中$h_0=[h_{L0}, h_{R2}]$,  $h_1 = [h_{L1}, h_{R1}]$,  $h_2=[h_{L2}, h_{R0}]$

![截屏2021-05-24 上午10.16.58](/Users/lixuanhong/Desktop/Materials/截屏2021-05-24 上午10.16.58.png)

+ 
  + 如果需要获取对句子的编码，则使用 $[h_L2, h_R2]$ 的拼接结果，因为这个包含了前向和后向的所有信息，如下图所示

    ![截屏2021-05-24 上午10.30.05](/Users/lixuanhong/Library/Application Support/typora-user-images/截屏2021-05-24 上午10.30.05.png)

5. GRU与LSTM的区别：

+ 它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态。最终的模型比标准的LSTM要更简单



### 工程或训练模型相关：

1. ab-test的意义：
   + 离线评估无法完全消除模型过拟合的影响。
   + 离线评估无法完全还原线上的工程环境。线上可能面临着延迟，数据丢失，标签数据缺失
   + 线上系统的某些商业指标在离线环境下不能计算
   
2. 如何进行线上的ab测试：
   + 对用户进行分桶，分成实验组和对照组，对实验组投放新模型，对对照组投放旧模型。分桶过程中要注意样本的独立性和无偏性
   + 注意用户特性，实验组和对照组除了模型不一样之外，其余用户特征应该是一样的。
   
3. 什么是过拟合吗？如何解决过拟合？如何判断过拟合？
   + 定义：对训练数据的拟合过当
   + 判断：在训练集上表现很好，在测试集，验证集上表现较差
   + 降低过拟合：
     + 获取更多的训练数据
     + 降低模型复杂度：减少网络层数，神经元个数等
     + 对参数使用正则化方法，即也要降低参数的损失
     + 集成学习方法：多个模型拼接到一起，降低单个模型的过拟合
   + 降低欠拟合：
     + 添加新特征（特征工程）：使用因子分解机，梯度提升决策树，deep-crossing
     + 增加模型复杂度
     + 减小正则化系数
   
4. softmax
   + 定义：能够将各输出节点的输出值映射到[0,1]，并且约束映射后的各节点输出值变成一个概率分布，输出值的和为1的函数
   + 对于某种意义上的hard-max，也就是直观的取出数组中的最大值。而在实际处理过程中，只找到一个最大值是不够的，我们更期望得到文章对于每个可能的文本类别的概率值（可以认为是某一个文章属于任意类别的可信度）
   + 含义：不再唯一确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值
   + 引入指数函数的优点：
     + x轴上的一个小变化，会带来y轴上较大变化
     + 常使用梯度下降进行参数更新，指数在求导比较方便
   + 引入指数的缺点：
     + 当数值很大时，可能会产生溢出（优化方式：在softmax公式中，将每一个输出值减去输出值中的最大值）
   + 根据softmax推导出的损失函数，与传统的损失函数有什么区别？
     + 对于softmax推导出的损失函数，只针对正确类别对应的输出节点，将这个位置的Softmax值最大化
     + 对于传统的损失函数，直接衡量真实分布和实际输出的分布之间的距离。
   
5. 调参

   + 寻找合适的学习率
     + batch越大，学习率越大
     + 在显存足够大的条件下，最好采用较大的 batch_size 进行训练，找到合适的学习率，然后加快收敛
     + 较大的 batch_size 可以避免 batch_normalization 一些问题

   + 权重初始化

     + 一般使用预训练的模型
     + kaiming_normal
     + Xavier_normal

   + dropout

     + 类似于 bagging ensemble 减小 variance，通过投票来减少可变性
     + 一般适用于全连接层，卷积层不太需要

   + 多模型融合

     + 同样的参数，不同的初始化方式

     + 不同的参数，通过cross-validation，选取最好的几组

     + 同样的参数，模型训练的不同阶段，即不同迭代次数的模型

     + 不同的模型，进行线性融合，例如RNN和传统模型

     + 提高性能和鲁棒性大法：

       + probs融合，投票法

       1. model1 probs + model2 probs + model3 probs ==> final label
       2. model1 label , model2 label , model3 label ==> voting ==> final label
       3. model1_1 probs + ... + model1_n probs ==> mode1 label, 同理 model2 label与model3 label方式与1相同 ==> voting ==> final label
   
6. AUC

   + 混淆矩阵
     + True Postive（TP，正样本且预测对），False Postive（负样本且预测错），
   
7. 解决梯度爆炸的方法：

   + 梯度裁剪：设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。
   
   + 权重正则化
   
   + 使用relu的激活函数
   
   + 使用 batch norm：通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布
   
     

### 减小复杂度相关

+ 词典与文本匹配
  + 落地：
    + 针对用户输入问题，快速找到问题中存在的实体
    + 在实体识别任务中，能够对原始文本进行实体标注
  + 对比
    + 方法1：对文本分词，然后判断分词结果是否在各个词典中。最坏时间复杂度为：O(mn)，其中m为词表大小，但是依赖于jieba分词的好坏，分词的结果可能是不准的；并且分词时间比较长
    + 方法2：遍历词典，一个词典中遍历词，检查当前词是否出现在文本中：最坏情况下时间复杂度：O(mn)，其中m为词表大小，带对数组的排序，O(nlogn)，字典数多了，匹配的速度就慢了
  + 选择方法：使用逆向最大匹配，在多个词典的情况下进行词典匹配的小效率最高；最坏情况下时间复杂度为 O(n^2)（亲测有效）
  + 作用：
    + 1、能够起到切词的作用，用于在新词发现时，通过ngram得到的词集对输入的文本进行切词
    + 2、可以用于在实体识别任务中实体位置标注，能够解决实体类型不同但名称相同的问题
    + 3、不需要获取切好的词在原文本中的索引，本身就是从后向前处理的，所以获取词后直接插入到结果数据0位置就可以得到切词的顺序



### 数学相关：

1. 极大似然估计（Maximum likelihood estimation）：

   + 用已知的样本信息，反推导致这些样本结果出现的最大概率的模型参数
   + <font color=red>看上去有点像一个无监督学习，一直观测状态，但是未知隐藏状态。总结来说，这个极大似然估计很像隐马尔可夫模型能够解决的学习问题，也就是无监督学习问题。但是极大似然估计是不存在隐变量的，所以也不能解决转移概率的问题。只能说他们之间只是思想相似</font>

   + 举例来说，通过所有可观测到`x`和`f(x)`的值，能够求出正态分布中的参数 $\mu$ 和 $\sigma$ ，那这个模型中所有的信息都可以获得
   + 假设：所有的采样都是独立同分布的
   + 对似然函数 $p(x|\theta)$ 的理解
     + 如果 $\theta$ 是已知确定的，$x$ 是变量，这个函数叫做概率函数 (probability function)，它描述对于不同的样本点 $x$ ，其出现概率是多少。
     + 如果 $x$ 是已知确定的，$\theta$ 是变量，这个函数叫做似然函数 (likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。
   + 参数的选择有无数种，具体要选哪一种，是要让这个样本出现的可能性最大（尽可能接近我们预测的值），这个最大可能性的结果就对应我们要找的那一套参数。
2. 介绍一下各种loss function

   + 对于分类问题：
     + 0-1损失：对于大于0的位置不惩罚，有不可导点
     + hinge损失：对于大于1的位置不惩罚，有不可导点
     + logistic损失：对于所有样本都有惩罚，处处可导，对异常值比较敏感
     + 交叉熵损失函数：针对预测值在[-1, 1]中，处处可导
   + 对于回归问题：
     + 平方损失：对于较远点惩罚力度大，比较敏感，
     + 绝对损失：预测值与真实值相等处不可导，对异常点比较鲁棒
     + Huber损失：在较小值处使用平方损失，在较大值处使用绝对损失
3. 常见的激活函数有哪些？都有什么特点？

   + Sigmoid：$f(z) = \frac {1}{1+exp(-z)}$，导数：$f(z)(1-f(z))$，
     + 可能出现梯度消失，在z很大/很小时
   + Tanh：$f(z) = tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$，导数：$1-f(z)^2$，
     + 可能出现梯度消失，在z很大/很小时
   + ReLU：$f(z) = max(0, z)$，导数：  $f(n) = \begin{cases} 1, & z>0 \\ 0, & z<=0 \end{cases}$
     + 优点：
       + 只要一个阈值就可以得到激活值，而不必像sigmoid和tanh那样计算指数
       + 非饱和性可以有效的解决梯度消失的问题，提供相对宽的激活边界
       + 单侧抑制提供了网络的稀疏表达能力
     + 局限性：
       + 负梯度在经过ReLU后被置为0，该神经元的梯度永远为0，不对任何数据产生响应
       + Leaky ReLU $f(x) = \begin{cases} z, z>0; \\ az, z <=0; \end{cases}$。一般a很小，这样既实现了单侧抑制，有保留了部分负梯度信息；但是a的值需要多次实验已确定
       + Parametric ReLU 将其中的a作为一个可学习参数，加入到模型中进行训练
       + 或者令a满足某种分布的随机采样
4. 为什么需要激活函数

   + 可能存在线性不可分的问题，需要非线性变换对数据的分布进行重新映射
   + 对于深度神经网络，添加激活函数能够避免多层网络等效于单层线性函数，使其能够获得更强大的学习于拟合能力
5. 如果数据不充足，或者不平均，要怎么解决？从数据增强和建模的方面解释（<font color=red> 数据不平衡问题 </font>）
   + 最简单的方式为随机采样
     + 随机过采样：从少数类样本集中随机重复抽取样本，有放回
     + 随机欠采样：从多数类样本集中随机选取较少的样本，有无放回均可
     + 过采样对少数样本进行多次复制，容易产生过拟合；欠采样可能会丢弃一些重要的信息，模型又可能只学到了整体特征的一部分
     + 可以使用==SMOTE==算法优化随机过采样。对少数类样本集中的每个样本x，从它在样本集中的K近邻中随机选取N个样本，在这样本x与这N个样本的连线上随机选取一个点作为新合成的样本（线性插值）。能够降低过拟合
     + 可以采用informed undersampling 来解决欠拟合的问题。常见的方法有Easy Ensemble，Balance Cascade，
   + 基于算法的优化方式
     + 改变模型训练时的目标函数来矫正这种不平衡性
     + 当样本数量及其不平衡时，也可以将问题转化为单类学习，异常检测等
6. 数据扩充的方法
   + 词汇替换：从句子中随机抽取一个或者多个单词，使用同义词进行替换。中文同义词可以训练一个word2vec来查找；英文同义词可以使用WordNet来查找同义词
   + 词embedding替换：直接计算词embedding的距离，然后用距离最近的N个词的embedding进行替换
   + MLM（Masked Language Model）：BERT、ROBERTA和ALBERT等模型已经在大量的文本上进行了训练，我们可以使用其进行文本的扩充，随机Mask个文本中的某个单词，然后使用模型对其进行预测。
   + 机器翻译：该策略的思路是先将原先的文本转化为其它回家的语义，然后再转化回来得到新的重新翻译之后的文本。该策略经常在一些小数据集的文本上被使用。
   + 文本表面转化：该转换的策略也非常简单，例如下面的就是She's ——> She has.
   + 错误注入拼写：我们将拼写错误添加到句子中的一些随机词中。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
   + QWERTY键盘错误注入：QWERTY键盘错误注入是直接模拟键盘输入出错的策略
   + 空格注入：这种思想是使用占位符标记替换随机单词。论文使用“_”作为占位符标记。
   + 句子Shuffliing：句子Shuffliing的策略就是对整个句子进行shuffle，切换句子的位置。
   + 交叉扩充：例如在tweet的文本情感中，一个tweet被分成两半，并且具有相同标签（正/负）的两个随机tweet被随机分开。假设是，即使结果不符合语法和语义，新的文本仍然会保留相同的情感。
7. 正则化

   + 参考资料：https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/

   + 为什么使用正则化：

     + 为了防止过拟合，增强泛化能力

   + 什么是正则化：从线性模型角度

     + 给训练的目标函数上加上一些规则，减小其过拟合情况的方法

     + 一般正则项：$\frac {\lambda}{2}\sum_{j=1}^M \vert\omega_j\vert^q$

       + M 表示模型的阶次，也就是数据的维度，当 M=2 时，也就是二位平面内的点
       + q = 2 就是二次正则项
       + ![q次正则项](/Users/lixuanhong/Desktop/Materials/q次正则项.png)

       + <font color=red>正则化能够防止过拟合的原因</font>：直观理解为目标函数的最小值，也就是求误差函数与正则项函数的和的最小值，这个值通常是两个曲面相交的地方
       + <font color=red>二次正则项的优势</font>：处处可导，限制模型复杂度，即 `w` 中的 `M` 的大小，`M`越小模型越简单
       + <font color=red>一次正则项的优势</font>：$w^*$ 的位置正好0（交点位置为0），相比二次正则项，维度降低更具体，更稀疏。但是一次正则项有拐点，不是处处可微的，但是可以通过一些技巧补充或避免这个问题

   + 什么是正则化：从神经网络的角度

     + 这里的 `M` 表示为神经网络中隐藏层中的神经元的数量
     + 数量越大，模型越复杂，过拟合的可能性越大
     + 未来保持神经网络的一致性（即输出的值不能被尺缩变换，或平移变换），在线性模型中的**加入正则项**无法奏效；**所以我们只能通过建立验证集（Validation Set），拉网搜索来确定`M`的取值（迭代停止的时间），又称为【提前停止】**

   + 范数：

     + **范数是具有“长度”概念的函数**，用于衡量一个**矢量的大小**（测量矢量的测度）
     + 如何表达“长度”这个概念是不同的，也就对应了不同的**范数**，本质上说，还是**观察问题的方式和角度不同**
     + 范数分为**向量范数**（二维坐标系）和**矩阵范数**（多维空间，一般化表达）
     + 针对向量范数：**0-范数：向量中非零元素的数量；1-范数：向量的元素的绝对值之和；2-范数：是通常意义上的模（向量元素的平方再开方，欧式距离）**
8. 优化器
   + 批量梯度下降
     + 优点：能够获得准确的梯度
     + 缺点：载入整个训练集进行计算，时间花费和内存开销都很大
   + 随机梯度下降
     + 优点：放弃准确的梯度，通过随机采样来计算梯度
     + 缺点：接受的信息量有限，对梯度的估计有偏差，使得目标函数收敛不稳定，甚至不收敛；容易在山谷中产生左右碰壁，在鞍点处梯度停留在水平位置，不能继续向下找到最优（山谷震荡，鞍点停滞）。
   + momentum（基于历史信息，保持惯性）
     + 优点：当前更新步伐除了学习率乘以当前梯度，还和上一步的更新步伐有关；可以理解为物理中求当前速度需要考虑到前一时刻的速度和当前的加速度；上一步更新步伐中的系数是衰减系数；带惯性的优化方法收敛速度更快，收敛曲线更稳定。
   + AdaGrad（对环境的感知，衡量梯度的稀疏性）
     + 文本建模中，词频比较小的对应参数更新频率低。但是我们希望更新频率低的参数可以拥有较大的更新幅度，而更新频率高的参数的幅度可以减小
     + 优点：历史梯度平方和，针对每个特定维度来更新学习率。衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。随着时间的推移，学习率越来越小，保证算法收敛。
   + Adam（惯性保持+环境感知）
     + 优点：记录一阶矩，过往梯度与当前梯度的平均，体现惯性保持；记录二阶矩，过往梯度的平方与当前梯度的平方的平均，体现环境感知。时间距离越近的过往梯度影响越大
     + 一阶矩：相当于估计当下梯度的期望，因为是随意采样的结果，因此更关注其在统计意义上的期望
     + 二阶矩：相当于估计当下梯度的平方的期望，这里与AdaGrad不同，不是累加，而是期望
     + 物理意义：
       + 一阶矩大，且二阶矩大，梯度大且稳定，前进方向明确
       + 一阶矩趋于零，且二阶矩大，梯度不稳定，可能遇到峡谷，容易引起反弹
       + 一阶矩，二阶矩都趋于零，可能达到局部最低，或平缓位置
