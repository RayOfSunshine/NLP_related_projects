## negative sampling 

### 引用及目的

+ 在 word2vec 中被提及

+ <font color=red>主要目的</font>：

  + skip-gram 中优化损失函数使用的是softmax，softmax会对词表上的所有词计算分值，复杂度比较高

    <font color=red>损失函数</font>：$loss = \frac {1}{T} \sum_{t=1}^T \sum_{-c \le j \le c, j \ne 0} log(P(w_{t+j}| w_t))$

    <font color=red>使用softmax计算概率分布</font>：$P(w_O,w_I) = \frac {exp(v_{w_O}^{'T} v_{w_I})} {\sum^W_{w=1}exp(v_{w}^{'T} v_{w_I}) }$，W 表示由语料产生的词表

  + 采用sigmoid激活函数+负采样，保证计算当前词分值尽可能大（概率最大化）的同时，其余词的分值尽可能小（概率最小化），减小计算量

    <font color=red>替换log概率分布</font>：$log\sigma(v_{w_O}^{'T} v_{w_I}) + \sum^k_{i=1} E_{w_i - P_n(w)}[log\sigma(-v_{w_i}^{'T} v_{w_I})]$

    + 前半部分属于预测正确的数据，通过中心词预测上下文的概率，采用sigmoid函数，越大越好
    + 后半部分属于噪声，即通过中心词预测k个噪生词的概率，越小越好

+ 负采样每次让训练样本仅仅更新一小部分的参数权重，从而降低梯度下降过程中的计算量

+ 假设此表大小为100，在结果中，希望输出的那一个词对应的节点的输出值为1，其余99个节点的输出值为0，这99个节点的词都称为【负词】，负采样就是随机选取一小部分的negative words，比如选择5个词作为更新对应的权重参数

+ <font color=red> 也就是说，原来需要更新100个神经元的节点，现在我只需要更新6（5个负样本和1个正样本）个神经元。~~和原来模型的区别仅仅是在于，更新的参数数量减少了，但是数据本身并没有改变~~ </font> 不对，应该是需要随机采集负样本的

### 负采样的选择

+ 使用<font color=red>一元模型分布</font>来选择negative sample，==一个单词被选作negative sample的概率跟它出现的频次有关，频次越高越容易被选择为【负词】==。这里需要使用这个公式来获取负例的词

  $P(\omega_i) = \frac {f(\omega_i)^{3/4}}{\sum_{j=0}^n{f(\omega_j)^{3/4}}}$ 其中，

  + $f(\omega)$ 表示词频
  + 分母表示所有单词的权重和

  

+ 原作者代码中统计了每个【实体】出现的频率，不知道是否和这个有关

+ 除此之外，原作者论文的公式中，指定了针对不同样本（正和负）对应的y值是不同的，说明应该需要【负样本】加入到模型训练中

  

### TensorFlow 中的负采样技术

+ tf.nn.sampled_softmax_loss

+ 对样本打y，y=1表示是正样本，y=0表示是负样本，而且针对的是观测值

+ 类别和标签的区别：

  