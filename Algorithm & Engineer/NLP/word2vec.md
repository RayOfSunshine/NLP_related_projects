## word2vec 相关

+ 是否考虑了词与词之间的顺序

  + 没有。上下文的词出现了就行，<font color=red>并没有考虑词与词之间的顺序</font>
  + 所以并不是最好的词嵌入，也不是深度学习算法

+ <font color=red>分布式假设</font>：

  + ==具有相同上下文语境的词应该具有相似的含义==

+ CBOW和skip-gram的区别

  + CBOW：根据上下文预测中间词
  + skip-gram：根据中间词预测上下文

+ 词向量表现在什么地方？

  + 训练的参数 $W_{n,v}$ 就是词向量矩阵（也就是n个词对应的v维的向量），输入词与输出词都是one-hot向量，从词向量矩阵中取出的一行就是该词对应的词向量

+ 短语的词嵌入；

  + 获得短语的词嵌入并不难，其实就是修改训练前的词典部分（“此词典非彼词典”中已经提过）把短语添加进去。如果你完全清楚要关注的短语有哪些，直接全部加到词典就行了。因此文章实际讨论的问题是在不知道文本中有哪些短语的情况下，如何在文本中找出来。

    $score(w_i, w_j) = \frac{count(w_i) - \delta}{count(w_i) - count(w_j)}$

    作者给出的解决方案非常简单，就是基于词频的统计，找出那些在一起经常出现而在其他上下文中不常出现的词组。

    该方法在文本上过第一次时可以找到两个词（word）组成的词组，在此基础上过第二遍就能找到三个词组成的词组，以此类推。
  
+ 训练结果的差异性

  + 预料影响较大
    + 相似的句子，相同部位的词会相似
    + 挨着近的词，也是相似的
  + 算法参数的影响
    + 对总体效果影响不大，比较重要的参数有如下几个
    + 降采样（采样频率降低）：
      + 降采样越低，对高频词越不利，对低频词有利
      + 降采样有类似tf-idf的功能，降低高频词对上下文影响的权重。
      + <font color=red>为什么要使用降采样？</font>
      + 一般设置成le-5。
    + 语言模型：
      + CBOW对频低的词更有利，因为 cbow是基于周围词来预测某个词
    + 窗口大小
      + 影响词和前后多少个词的关系，和语料中语句长度有关，需要统计语料中，句子长度的分布
      + 一般设置成8。
    + min-count
      + 设置相对大一点的 min-count 过滤掉切错的词，毕竟切错的是少数情况，使得这种错词词频不高
    + 向量维度
      + 如果词量大，训练得到的词向量还要做语义层面的叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。
      + 一般情况下200维

+ word2vec 影响速度的因素有哪些？

  + 语言模型
    + cbow比skip-gram快，因为它是上下文预测中间词，而skip-gram是中间词预测上下文
    + cbow只需要一次运算（ 窗口内的词向量求平均），而skip-gram的运算量与窗口大小有关，窗口越大，运算量越大
  + 迭代次数
    + 影响训练次数，预料不够的情况下，可以调大迭代次数
  + 线程数
    + 单机版（google word2vec)可以通过设置多线程跑
    + 集群版（spark mllib）可以设置多个 partitions，但是 partitions 过多，会影响训练的效果。

+ 评估word2vec训练的好坏

  + 可以使用kmeans聚类，看聚类簇的分布
  + 查找cos相近的词
  + 词分布可以通过tensorboard查看
  
+ 优化方式：

  + 原始softmax的求解公式是：$p(o|c) = \frac {exp(u_o^Tv_c)}{\sum_{w=1}^Vexp(u_w^Tv_c)}$，这是在整个词表上进行softmax，也就是对所有的词进行softmax计算，复杂度很高，softmax要输出V的概率，所以需要优化
  
  + <font color=red>hierarchical softmax</font>
  
    + 将softmax求解V个概率，转化为sigmoid求解 $log_2V$  概率，也就是 $log_2V$ 个sigmoid计算，能够减少复杂度，具体实现方式可以使用满二叉树
    + <font color=red>更快的方案：使用一颗二叉树表示词汇表中的单词，每个单词都作为二叉树的叶子节点，树为最优二叉树，带权路径长度最短的二叉树 （ 词频大的更靠近根节点，词频小的远离根节点）</font> 。
    + 对于一个大小为V的词汇表，其对应的二叉树包含V-1非叶子节点（这里应该是考虑了文本进去，文本中最后一个单词是二叉树中的叶子节点，那么剩下的V-1就是非叶子节点）。
    + 针对skip-gram，huffman树求解sigmoid计算少于 $log_2V$ 次，有效的减少词向量。实际huffman树上每个节点上的是可训练的参数 $\theta$，也就是上下文词向量
    + sigmoid函数性质：$\sigma(-x) = 1 - \sigma(x)$
    + $p(w|w_I) = \prod_{j = 1}^{L(w)-1} \sigma([n(w, j+1)==ch(n(w,j))]*v_{n(w,j)}^{`T}v_{w_I})$ -- <font color=red>重要</font>
      + 其中 $n(w,j)$ 表示词 $w$ 在树上的第j个节点，$L$ 为树高；
      + $[n(w, j+1)==ch(n(w,j))]$，<font color=red>表示第 j+1 个节点是否是第 j 个节点的右子节点</font>，如果是那么 $[x] = 1$，否则等于 $[x] = -1$，对应于huffman树向左走和向右走不同的编码方式。
      + $v_{w_{I}}$ 是已知的中心词词向量表达；$v_{n(w,j)}^{`}$ 为每个内部节点的唯一词向量表达，也就是词 $w$ 在树上的第j个节点的参数，即上下文词向量，通过前面的函数的正负结果来影响走向与上下文向量的和差结合
    + huffman树的性质：
      + 每个非叶子节点向左转标记为0，向右转标记为1，那么每个单词都具有唯一的从根节点到达该叶子节点的由｛0 1｝组成的代号
      + **实际上为哈夫曼编码，为哈夫曼树，是带权路径长度最短的树（所有叶子节点的权值乘上其到根节点路径的长度的累加和最小），哈夫曼树保证了词频高的单词的路径短（词频就是相应的权值，也就是一个节点表示的值，节点上的值），词频相对低的单词的路径长，这种编码方式很大程度减少了计算量**
      + https://zhuanlan.zhihu.com/p/56139075
      + 每一个节点上的值等于其左右子节点上值的和，在构造过程中，先合并值小的节点，后合并值大的节点
    + 针对CBOW模型，其中的 $u_0$ 表示的是窗口内上下文词向量的平均。需要注意的是在skip-gram和CBOW中，词向量的个数是少于V的，那么词向量的求解方式就不能通过中心词向量和周围词向量的求和与平均来计算了
  
  + Negative sampling
    + **为每个训练实例都提供负例**
    + 实际上就是一个**带权采样**过程，负例的选择机制是和单词词频联系起来的。
  
    + <font color=red>主要目的</font>：
  
      + skip-gram 中优化损失函数使用的是softmax，softmax会对词表上的所有词计算分值，复杂度比较高
  
        <font color=red>损失函数</font>：$loss = \frac {1}{T} \sum_{t=1}^T \sum_{-c \le j \le c, j \ne 0} log(P(w_{t+j}| w_t))$
  
        <font color=red>使用softmax计算概率分布</font>：$P(w_O,w_I) = \frac {exp(v_{w_O}^{'T} v_{w_I})} {\sum^W_{w=1}exp(v_{w}^{'T} v_{w_I}) }$，W 表示由语料产生的词表；计算量太大，所以需要舍弃多分类，减小计算量，提升速度
  
      + 采用二分类的思想，即sigmoid激活函数 + 负采样，保证计算当前词分值尽可能大（正样本概率最大化）的同时，其余词的分值尽可能小（负样本概率最小化），减小计算量，一般选择5个负样本
  
        <font color=red>替换log概率分布的目标函数</font>：$log\sigma(v_{w_O}^{'T} v_{w_I}) + \sum^k_{i=1} E_{w_i - P_n(w)}[log\sigma(-v_{w_i}^{'T} v_{w_I})]$
  
      + 前半部分属于预测正确的数据，通过中心词预测上下文的概率，采用sigmoid函数，越大越好
  
      + 后半部分属于噪声，即通过中心词预测k个噪生词的概率，越小越好
  
      + <font color=red>这里还是需要每个词的上下文词向量，总的参数比层次softmax多，但是计算量小，只需要计算k+1个概率，其中k是负样本的个数</font>；层次softmax的要输出 $log_2V$ 个概率
  
    + 负采样每次让训练样本仅仅更新一小部分的参数权重，从而降低梯度下降过程中的计算量
  
    + 假设此表大小为100，在结果中，希望输出的那一个词对应的节点的输出值为1，其余99个节点的输出值为0，这99个节点的词都称为【负词】，负采样就是随机选取一小部分的negative words，比如选择5个词作为更新对应的权重参数
  
    + <font color=red> 也就是说，原来需要更新100个神经元的节点，现在我只需要更新6（5个负样本和1个正样本）个神经元。~~和原来模型的区别仅仅是在于，更新的参数数量减少了，但是数据本身并没有改变~~ </font> 不对，应该是需要随机采集负样本的
  
    + 负采样的选择
  
      + 使用<font color=red>一元模型分布</font>来选择negative sample，==一个单词被选作negative sample的概率跟它出现的频次有关，频次越高越容易被选择为【负词】==。这里需要使用这个公式来获取负例的词
  
        $P(\omega_i) = \frac {f(\omega_i)^{3/4}}{\sum_{j=0}^n{f(\omega_j)^{3/4}}}$ 其中，
  
        + $f(\omega)$ 表示词频
        + 分母表示所有单词的权重和
        + 在这个公式下，能够使得频率大的词的采样概率越大，频率小的词的采样概率越小。因为频率较大的词带有的语义信息是不够的，更适合作为负样本，训练效果更好
  
  + 重采样（subsampling of frequent words）
  
    + 词频比较高的词携带的信息比较少
    + 原因：更多的训练重要的词对，高频词训练时间比较短，低频次需要更长的时间进行训练
    + 方法：$P(w_i) = 1 - \sqrt \frac {t}{f(W_i)}$，
      + $f(w_i)$ 为词 $w_i$ 在数据集中出现的频率，文中t选取为10的-5，训练集中的词 $w_i$ 会以 $P(w_i)$ 的概率被删除。
      + 词频越大，$f(w_i)$ 越大， $P(w_i)$ 越大，词 $w_i$ 会以更大的概率被删除
      + 优点：加速训练，能够得到更好的词向量



### TensorFlow 中的负采样技术

+ tf.nn.sampled_softmax_loss

+ 对样本打y，y=1表示是正样本，y=0表示是负样本，而且针对的是观测值

+ 类别和标签的区别：

  