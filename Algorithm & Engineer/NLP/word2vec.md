## word2vec 相关

+ 是否考虑了词与词之间的顺序

  + 没有。上下文的词出现了就行，<font color=red>并没有考虑词与词之间的顺序</font>
  + 所以并不是最好的词嵌入，也不是深度学习算法

+ <font color=red>分布式假设</font>：

  + ==具有相同上下文语境的词应该具有相似的含义==

+ CBOW和skip-gram的区别

  + CBOW：根据上下文预测中间词
  + skip-gram：根据中间词预测上下文

+ 词向量表现在什么地方？

  + 训练的参数 $W_{n,v}$ 就是词向量矩阵（也就是n个词对应的v维的向量），输入词与输出词都是one-hot向量，从词向量矩阵中取出的一行就是该词对应的词向量

+ 短语的词嵌入；

  + 获得短语的词嵌入并不难，其实就是修改训练前的词典部分（“此词典非彼词典”中已经提过）把短语添加进去。如果你完全清楚要关注的短语有哪些，直接全部加到词典就行了。因此文章实际讨论的问题是在不知道文本中有哪些短语的情况下，如何在文本中找出来。

    $score(w_i, w_j) = \frac{count(w_i) - \delta}{count(w_i) - count(w_j)}$

    作者给出的解决方案非常简单，就是基于词频的统计，找出那些在一起经常出现而在其他上下文中不常出现的词组。

    该方法在文本上过第一次时可以找到两个词（word）组成的词组，在此基础上过第二遍就能找到三个词组成的词组，以此类推。
  
+ 训练结果的差异性

  + 预料影响较大
    + 相似的句子，相同部位的词会相似
    + 挨着近的词，也是相似的
  + 算法参数的影响
    + 对总体效果影响不大，比较重要的参数有如下几个
    + 降采样（采样频率降低）：
      + 降采样越低，对高频词越不利，对低频词有利
      + 降采样有类似tf-idf的功能，降低高频词对上下文影响的权重。
      + <font color=red>为什么要使用降采样？</font>
      + 一般设置成le-5。
    + 语言模型：
      + CBOW对频低的词更有利，因为 cbow是基于周围词来预测某个词
    + 窗口大小
      + 影响词和前后多少个词的关系，和语料中语句长度有关，需要统计语料中，句子长度的分布
      + 一般设置成8。
    + min-count
      + 设置相对大一点的 min-count 过滤掉切错的词，毕竟切错的是少数情况，使得这种错词词频不高
    + 向量维度
      + 如果词量大，训练得到的词向量还要做语义层面的叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。
      + 一般情况下200维

+ word2vec 影响速度的因素有哪些？

  + 语言模型
    + cbow比skip-gram快，因为它是上下文预测中间词，而skip-gram是中间词预测上下文
    + cbow只需要一次运算（ 窗口内的词向量求平均），而skip-gram的运算量与窗口大小有关，窗口越大，运算量越大
  + 迭代次数
    + 影响训练次数，预料不够的情况下，可以调大迭代次数
  + 线程数
    + 单机版（google word2vec)可以通过设置多线程跑
    + 集群版（spark mllib）可以设置多个 partitions，但是 partitions 过多，会影响训练的效果。

+ 评估word2vec训练的好坏

  + 可以使用kmeans聚类，看聚类簇的分布
  + 查找cos相近的词
  + 词分布可以通过tensorboard查看
  
+ 优化方式：

  + <font color=red>hierarchical softmax</font>
    + 使用一颗二叉树表示词汇表中的单词，每个单词都作为二叉树的叶子节点。
    + 对于一个大小为V的词汇表，其对应的二叉树包含V-1非叶子节点（这里应该是考虑了文本进去，文本中最后一个单词是二叉树中的叶子节点，那么剩下的V-1就是非叶子节点）。
    + 每个非叶子节点向左转标记为1，向右转标记为0，那么每个单词都具有唯一的从根节点到达该叶子节点的由｛0 1｝组成的代号
    + **实际上为哈夫曼编码，为哈夫曼树，是带权路径长度最短的树，哈夫曼树保证了词频高的单词的路径短，词频相对低的单词的路径长，这种编码方式很大程度减少了计算量**
  + Negative sampling
    + **为每个训练实例都提供负例**
    + 实际上就是一个**带权采样**过程，负例的选择机制是和单词词频联系起来的。
    + 