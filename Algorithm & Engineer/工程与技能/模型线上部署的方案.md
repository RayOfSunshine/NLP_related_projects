1. 自研平台：

   + 为什么不直接使用TensorFlow，而要从头到尾进行模型和平台自研
   + TensorFlow等通用平台为了灵活性和通用性支持大量冗余的功能，导致平台过重，难以修改和定制。而**自研平台的好处是可以根据公司业务和需求进行定制化的实现，并兼顾模型serving的效率**。
   + 从参数服务器或内存数据库中得到模型参数
   + 实现模型的时间成本较高，迭代周期过长。因此自研平台和模型往往只在大公司采用，或者在已经确定模型结构的前提下，手动实现inference过程的时候采用。

2. 预训练embedding+轻量级模型

   + **复杂网络离线训练，生成embedding存入内存数据库，线上实现LR或浅层NN等轻量级模型拟合优化目标**
   + 分别用复杂网络对“用户特征”和“广告特征”进行了embedding化，在最后的交叉层之前，用户特征和广告特征之间没有任何交互

   + 而在线上inference时，也不用复现复杂网络，只需要实现最后一层的逻辑，在从内存数据库中取出用户embedding和广告embedding之后，通过简单计算即可得到最终的预估结果。

3. PMML

   + 是一种一种脱离于平台的通用的模型部署方式，能够实现End2End训练+End2End部署
   + 预测模型标记语言（Predictive Model Markup Language）**在模型上线的过程中，PMML经常作为中间媒介连接离线训练平台和线上预测平台**。

   ![spark模型利用PMML的上线过程](/Users/lixuanhong/Desktop/spark模型利用PMML的上线过程.png)

   + 例子使用了JPMML作为序列化和解析PMML文件的library。JPMML项目分为Spark和Java Server两部分。Spark部分的library完成Spark MLlib模型的序列化，生成PMML文件并保存到线上服务器能够触达的数据库或文件系统中；Java Server部分则完成PMML模型的解析，并生成预估模型，完成和业务逻辑的整合。
   + 由于JPMML在Java Server部分只进行inference，不用考虑模型训练、分布式部署等一系列问题，因此library比较轻，能够高效的完成预估过程。与JPMML相似的开源项目还有Mleap，同样采用了PMML作为模型转换和上线的媒介。

4. TensorFlow serving 等原生serving平台

   + 对于较为复杂的模型，PMML的语言能力不够，上线的tensorflow模型需要tensorflow serving的原生平台

   + 本质上讲TensorFlow Serving的工作流程和PMML类的工具的流程是一致的。不同之处在于TensorFlow定义了自己的模型序列化标准。利用TensorFlow自带的模型序列化函数可将训练好的模型参数和结构保存至某文件路径。

   + TensorFlow Serving最普遍也是最便捷的serving方式是使用Docker建立模型Serving API。在准备好Docker环境后，仅需要pull image即可完成TensorFlow Serving环境的安装和准备。

     ```dockerfile
     docker pull tensorflow/serving
     
     # 启动该 docker container 后，也仅需要一行命令就可以启动模型的serving api
     tensorflow_model_server   
     	--port=8500  \  
     	--rest_api_port=8501 \
         --model_name=${MODEL_NAME}   
         --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}
     ```

     这里仅需注意之前保存模型的路径即可。

   + TensorFlow Serving的性能问题也仍被业界诟病。但Tensorflow Serving的易用性和对复杂模型的支持仍使其是上线TensorFlow模型的第一选择。

5. 公司常见方案：

   1. 直接在tf-serving上二次开发成算法平台的

   2. 我们是把模型通过freeze graph的方式生成一个常量的graph，线上通过C++的tf lib直接load graph，通过session run来feed input fetch output，比tf serving快好多，需要词表这种依赖也是没问题的，会init表。

   3. 模型线上服务其实是一个系统工程，要考虑的东西很多，我简单列几个：

      模型热启动，启动前需要warmup下模型定时加载，优雅过渡，不要出现突刺模型线上指标计算，现在很多人只是离线跑了下，根本不去关注线上指标，其实过拟合了都没发现。我们改造成一个小时数据就用线上的模型预测一次，实时监控起来模型定时更新，不更新的复杂模型只能用来发论文和晋升