### 炼丹

+ 训练集，验证集，测试集
  + 验证集的作用
    + 评估模型效果，为了调整超参数而服务
    + 调整超参数，使得模型在验证集上的效果最好
  + 最终的模型结果由测试集进行评估
+ 交叉验证
  + 训练集和测试集完全不同的验证方法就是交叉验证法
  + 留出法
    + 按照固定比例将数据集**静态的**划分为训练集、验证集、测试集。的方式就是留出法。
  + 留一法
    + 每次的测试集都只有一个样本，要进行 m 次训练和预测。 这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。 一般在数据缺乏时使用。
  + k折交叉验证--动态验证的方式
    + 将数据集分为训练集和测试集，将测试集放在一边
    + 将训练集分为 k 份
    + 每次使用 k 份中的 1 份作为验证集，其他全部作为训练集。
    + 通过 k 次训练后，我们得到了 k 个不同的模型。
    + 评估 k 个模型的效果，从中挑选效果最好的超参数
    + 使用最优的超参数，然后将 k 份数据全部作为训练集重新训练模型，得到最终模型。
    + 数据量小的时候，k 可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。 数据量大的时候，k 可以设小一点。
+ <font color=red>train loss 和 test loss 比较分型模型情况</font>
  + train loss 不断下降，test loss不断下降，说明网络仍在学习;
  + train loss 不断下降，test loss趋于不变，说明网络过拟合;
  + train loss 趋于不变，test loss不断下降，说明数据集100%有问题;
  + train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;
  + train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。

+ 寻找合适的学习率
  + 不确定性比较大，在训练中不断寻找最合适当前状态的学习率
  + 参考论文：
    + How Do You Find A Good Learning Rate
    + Cyclical Learning Rates for Training Neural Networks
  + learning-rate 和 batch-size 的关系
    + 越大的batch_size需要越大的学习率，因为其收敛方向是很确定的
    + 小的batch_size是比较乱的，没有规律
    + 显存足够的条件下，应采用较大batch_size训练，找到合适的lr，加快收敛
    + 较大的batch-size可以避免batch normalization出现的一些小问题
+ 权重初始化
  + 直接使用预训练模型，权重都是在大型数据集上训练好的模型
  + 常用的权重初始化算法是 **「kaiming_normal」** 或者 **「xavier_normal」**
  + 参考论文：
    + Understanding the difficulty of training deep feedforward neural networks
    + Xavier初始化论文
    + He初始化论文
  + uniform 初始化
  + Xavier 初始化，适用与普通的激活函数
  + He 初始化，适用于ReLU
  + normal 高斯分布初始化，均值为0，使用高斯分布的标准差
  + svd 初始化，对 RNN 效果比较好
+ dropout
  + 随机丢弃一些神经元，那么每一个mini-batch都在训练不同的网络
  + 类似于通过投票来减少可变性，类似于bagging ensembel
  + 在全连接的网络中使用较大概率的dropout；CNN中不用，因为参数不多
+ 数据集处理
  + 数据筛选
  + 数据增强
+ 难例挖掘
  + 分析模型难以预测正确的样本，给出针对性方法。
+ <font color=red>多模型融合</font>
  + ensemble
    + 同样的参数，不同的初始化方式
    + 不同的参数，通过cross-validation，选取最好的几组
    + 同样的参数，模型训练的不同阶段，即不同迭代次数的模型。
    + 不同的模型,进行线性融合. 例如RNN和传统模型
  + 提高模型性能和鲁棒性
    + probs融合
      + model1 probs + model2 probs + model3 probs ==> final label
    + 投票法
      + model1 label , model2 label , model3 label ==> voting ==> final label
    + 综合
      + model1_1 probs + ... + model1_n probs ==> mode1 label, model2 label与model3获取的label方式与1相同 ==> voting ==> final label
+ 差分学习率和迁移学习
  + 迁移学习：
    + 利用很多预训练的经典模型直接去训练我们自己的任务。虽然说领域不同，但是在学习权重的广度方面，两个任务之间还是有联系的。
    + 我们直接拿来其他任务的训练权重，在进行optimize的时候，如何选择适当的学习率是一个很重要的问题。
  + 差分学习率
    + 在不同的层设置不同的学习率，可以提高神经网络的训练效果
    + 参考论文：
      + https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00
+ 余弦退火(cosine annealing)和热重启的随机梯度下降
  + 余弦退火：学习率类似余弦函数慢慢下降
  + 热重启：学习率慢慢下降然后突然再回弹 (重启) 然后继续慢慢下降。
  
  + 参考论文：Stochastic Gradient Descent with Warm Restarts
+ 尝试过拟合一个小数据集

  + 关闭正则化/随机失活/数据扩充，使用训练集的一小部分，让神经网络训练几个周期。确保可以实现零损失，如果没有，那么很可能什么地方出错了。
+ 交叉验证

  + 解决数据不充足的问题，基本目的就是重复使用数据
  + 在平常中我们将所有的数据分为训练集和验证集就已经是简单的交叉验证了，可以称为1折交叉验证。【测试集和交叉验证没关系，测试集是用来衡量我们的算法标准的，不参与到交叉验证中来。】
  + 我们经常使用的是5-折(5-fold)交叉验证，将训练集分成5份，随机挑一份做验证集其余为训练集，循环5次，这种比较常见计算量也不是很大。
  + 还有一种叫做 `leave-one-out cross validation` 留一交叉验证，这种交叉验证就是n-折交叉，n表示数据集的容量，这种方法只适合数据量比较小的情况
+ 优化算法：

  + 解决问题：有时 loss 降不下去，换 Adam 瞬间就好了
  + 带来问题：比如单词词频差异很大，当前 batch 没有的单词的词向量也被更新；再比如Adam和L2正则结合产生的复杂效果
  + adam的相比SGD，收敛快，但泛化能力差，更优结果似乎需要精调SGD。
  + 据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。adam收敛虽快但是得到的解往往没有sgd+momentum得到的解更好，如果不考虑时间成本的话还是用sgd吧。
  + 如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半.
+ 数据预处理方式

  + zero-center ,这个挺常用的：`X -= np.mean(X, axis=0)`
+ 标签平滑
  + 将hard label转变为soft label，是网络优化更加平滑。
  + 标签平滑是用于深度神经网络（DNN）的有效正则化工具，该工具通过在均匀分布和hard标签之间应用加权平均值来生成soft标签。它通常用于减少训练DNN的过拟合问题并进一步提高分类性能。
  + `targets = (1 - label_smooth) * targets + label_smooth / num_classes`
+ 训练技巧

  + 要做梯度归一化,即算出来的梯度除以minibatch size
  + clip c(梯度裁剪): 限制最大梯度，其实是 $value = sqrt(w_1^2+w_2^2….)$​，如果value超过了阈值，就算一个衰减系系数，让value的值等于阈值: 5,10,15
  + dropout对小数据防止过拟合有很好的效果,值一般设为0.5
    + 小数据上首先尝试 dropout + sgd 效果提升都非常明显
  + dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout
    + 参考论文：http://arxiv.org/abs/1409.2329
  + 除了gate之类的地方，需要把输出限制成0-1之外，尽量不要用sigmoid，可以用tanh或者relu之类的激活函数.
  + rnn 的 dim 和 embdding size 一般从128上下开始调整。 batch size 一般从128左右开始调整。batch size合适最重要，并不是越大越好.
  + word2vec初始化，在小数据上，不仅可以有效提高收敛速度，也可以提高结果.
  + 尽量对数据做shuffle
  + LSTM 的forget gate的bias，用1.0或者更大的值做初始化，可以取得更好的结果
    + 参考论文：http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf
  + Batch Normalization据说可以提升效果
    + 参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift
  + 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动
    + 参考论文：http://arxiv.org/abs/1505.00387
  + 一轮加正则，一轮不加正则，反复进行。
  + 在数据集很大的情况下，一上来就跑全量数据。建议先用 1/100、1/10 的数据跑一跑，对模型性能和训练时间有个底，外推一下全量数据到底需要跑多久。在没有足够的信心前不做大规模实验。
  + subword 总是会很稳定地涨点，只管用就对了。
  + GPU 上报错时尽量放在 CPU 上重跑，错误信息更友好。
  + 在确定初始学习率的时候，从一个很小的值（例如 1e-7）开始，然后每一步指数增大学习率（例如扩大1.05 倍）进行训练。训练几百步应该能观察到损失函数随训练步数呈对勾形，选择损失下降最快那一段的学习率即可。
  + 注意实验的可复现性和一致性，注意养成良好的实验记录习惯
  + 超参上，learning rate 最重要，推荐了解 cosine learning rate 和 cyclic learning rate，其次是 batchsize 和 weight decay。当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。