#### 实体链接（实体对齐）：

+ 实体对齐：（需要考虑上下文的）
  + 含义：将非结构化数据中的表示实体的词语(即所谓mention，对某个实体的指称项)识别出来，并将从知识库(领域词库，知识图谱等)中找到mention所表示的那一个实体
  + 表现1：实体消岐：根据上下文信息实现消除一词多义的歧义现象（同一mention对应不同的实体）
  + 表现2：共指消解：解决多词一义（同一实体有不同的mention）
    + 实体对齐：从各种信息源上聚集实体信息，使实体包含的信息更丰富，更准确，并在指向相同实体之间构建对齐关系
    + <font color=red>实体对齐是无向的，指代消岐是有向的</font>
  + mention 识别：序列标注的问题
  + 寻找mention指向的实体，使用<font color=red>召回排序</font>
    + 使用规则或模型，生成(数量远小于知识库规模的)若干候选实体。
      + 同义词表：需要处理未收录词的问题
      + 将实体名称以字或ngram为key的倒排索引中，然后把mention当做一个查询语句、把实体当做目标文档，进行检索；
      + 使用LSA、词嵌入等方法表示词语，并在此基础上进行召回；等等。
    + 计算mention与每个候选实体的匹配度，然后选择匹配度最高的作为mention指向的实体。
      + 构造mention与候选实体的表示（基于短文本匹配，需要考虑所在文本的语义信息）
      + 使用有监督或无监督方法计算mention与候选实体的匹配度。我们的武器库非常丰富，包含了距离计算方法、分类模型、聚类算法、排序模型等等都可以用。
    + mention是“无法链接”(Unlinkable)的

+ 实体规范化：只给mention，没有上下文

  + 召回排序策略

    + 给定m从KB中做召回，通过信息检索模型（比如elastic search）找出一个候选子集 {c1, c2, ..., cn} （注意，这里要求给已有的实体配足够多和丰富的别名。对于每一个entity，紧凑而充分地配置别名，才能保证生成的candidate entites没有遗漏掉ground truth entity。）
    + 还有一些方法：诸如 generating + re-ranking 的方法，也就是使用生成式模型来筛选候选子集
    + 使用字符串相似度算法，如BM25或tf-idf等，采用排序模型对候选子集做相似度排序，相似度最高的 ci 作为标准化名称，排序模型可以使用sentence-pair，也就是 m 和 c 的 pair，即文本相似度排序
    + <font color=red>这里使用 BM25 算法做召回，采用 esim 算法做排序</font>
    + 数据中共有两列，分别是：原词 = 待对齐的词 M，规范词 = KB中的概念C
    + 训练集：
      + 正样本：将每行每列包含的词进行拆分，最终的词表为每行只有两个词，也就是原词与规范词的一一对应，所有的数据作为正样本；
      + 负样本：针对每个原词，从实体库（code.txt）中找到一些词与原词配对组成负样本。负样本的选取方式可以使用随机选取，但是效果不好；可以采用bm25算法计算原词与实体词的相似度，然后选择最相似的topK，并且从中排除掉与对象原词所对应的规范词，最终的得到结果为k-1个配对；实验中召回10个负样本
    + 测试集：
      + 将验证集上每行词进行拆分，与构造正样本类似，得到一一对应的配对结果。针对每一个原词，使用模型从KB库，也就是 code.txt 里，去召回预测一个规范化实体作为预测值，然后比较预测值和表中原词对应的规范词是否相同，然后将相同的数量除以总原词数量得到准确率
    
    



#### 实体消岐

+ 一词多义
  + 同一个词在不同语言环境下表达的意思不同
  + 实现：包含待消岐实体的句子转化为向量 $V_t$​​，包含同一实体且语义为1的句子向量转化为 $V_1$​​， 包含同一实体且语义为2的句子向量转化为 $V_2$​​， 我们只要将 $V_t$​​ 分别与 $V_1$​​，计算相似度 $V_2$​​，然后对比 $sim(V_1, V
    _t)$​ 和 $sim(V_2, V
    _t)$​；相似度高的，我们则将其看作“苹果”的真实语义
+ 不同词在相同语言环境下表示相同的意思
  + 也可以使用类似的处理方法，计算整个context上的语义，然后比较不同context上的相似度



#### 共指消歧

+ 指代消解。必须使用多种信号和知识来消除歧义。分为两个步骤
  + 指代识别（mention detection）：找出句子中所有的指代
    + 保召回率，保留所有找到的可能是指代的词，都参与后期的指代消解。
    + 如果一个指代没有找到它的共同指代（coreference），则说明这个指代是孤立的（singleton mention），有可能是指代识别阶段找到的不是指代的词，直接舍弃。
  + 指代消解（coreference resolution，也就是共指解法）
    + Rule-based
      + 该领域的baseline模型
    + Mention pair
      + 该方法把指代消解问题转化为一个二分类问题。从左到右遍历句子，每找到一个指代，就把它和前面找到的每个指代作为一个pair，问分类器这个pair是否指代同一个实体，如果是的话，就把它们连起来。二分类的损失就是交叉熵。很简单的一个模型。
    + Mention Ranking
      + 每个指代同时和前面所有指代（还是先行词？）打分，用softmax归一化，找出概率最大的先行词（即指代指向的词），添加一条连边。注意需要添加一个NA节点，因为有的指代可能第一次出现，前面没有先行词，或者这个指代根本就不是一个真正的指代。
    + 计算两个指代coreference（共指）概率的方法：
      + Non-neural statistical classifier。统计机器学习方法，抽取每个指代的各种特征，然后用机器学习分类器来计算两个指代是coreference的概率。这里面的特征包括人称、性别一致性，语义相容性等等。
      + Neural Coref Model。输入是候选先行词和当前指代词的词向量，还需要加入一些额外的特征（Additional Feature），也就是上面统计机器学习方法里用到的一些特征。中间是FFNN（fully_forward_neural network），即全连接网络，最后输出两个指代是coreference的概率。
      + End-to-end Model。end2end模型是目前指代消解的SOTA模型，它把指代识别和指代消解两个任务融合到一起，用一个模型来解决。