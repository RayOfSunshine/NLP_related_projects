新词发现

1. 无监督构建词库：更快更好的新词发现算法

+ 基础任务：通过无监督发掘语言特征（主要是统计特征）
+ 分词的目的
  + 人本来就是按照字来书写和理解的
  + 模型能力足够强，可以使用基于字的模型进行训练和拟合，但是模型会复杂而导致效率下降
  + 简单高效的模型，对文本分类任务来说，可以是朴素贝叶斯分类器（特征独立，假设性较强），或FastText。注意到，当特征之间明显不独立的时候，可以考虑将特征组合之后，使得特征之间的相关性减弱，再用朴素贝叶斯。举例来说就是字与字进行组合，形成词后，词与词之间的相关性会降低
  + 所以可以认为，对句子分词，实际上是削弱句子的相关性，降低了对词序的依赖，也就是说<font color=red>分的可能不一定是“词”，也可能是短语、常用搭配等</font>。如果分得过细，就会破坏掉原有句子中的信息

+ 算法步骤：
  + 统计，选取某个固定的 n，统计 2grams、3grams、…、ngrams，计算它们的内部凝固度，只保留高于某个阈值的片段，构成<font color=red>集合 G</font> （不同的n对应着不同的集合G）；这一步，可以为 2grams、3grams、…、ngrams 设置不同的阈值。因为字数越大，一般来说统计就越不充分，越有可能偏高，所以字数越大，阈值要越高；
    + 凝固度的表示（以n=3为例）：$min(\frac {P(abc)}{P(a)P(bc)}, \frac {P(abc)}{P(ab)P(c)})$
    + 集合G可以作为一个中间词表，不是最后发现的新词
  + 切分：<font color=red>用上述 grams 的结果集合G 对语料进行切分（粗糙的分词），并统计频率</font>。切分的规则是，只有一个片段出现在前一步得到的集合 G 中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在 G 中，这时候就算“各项目”不在 G 中，那么“各项目”还是不切分，保留下来；
    + 宁放过，勿切错
    + 虽然这样的分词比较粗糙，但高频的部分还是靠谱的，所以筛选出高频部分；
  + 回溯：经过第二步，“各项目”会被切出来（因为第二步保证宁放过，不切错）。回溯就是检查，如果它是一个小于等于 n 字的词，那么检测它在不在 G 中，不在就出局；如果它是一个大于 n 字的词，那个检测它每个 n 字片段是不是在 G 中，只要有一个片段不在，就出局。还是以“各项目”为例，回溯就是看看，“各项目”在不在 3gram中，不在的话，就得出局（本身就不希望“各项目”成词）。
    + 这里可以使用字典树
+ 参考：https://zhuanlan.zhihu.com/p/82142692